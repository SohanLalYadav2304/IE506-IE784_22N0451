{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For the given function f($\\mathbf{x}$) = $400x_1^2 + 0.004x_2^4$\n",
        "\n",
        "  Since the given function is differentiable and its second order partial derivatives exist and are continuos, we may find the hessian matrix for it and it comes out to be as follows: \n",
        "$$\\nabla^2f(\\mathbf{x}) = H = \\begin{bmatrix}\n",
        "800 & 0 \\\\ 0 & 0.048x_2^2\\end{bmatrix}$$\n",
        "Thus we take $D^k$ as following:\n",
        "\n",
        "\n",
        "$\\mathbf{D^k} =\n",
        "\\begin{bmatrix}\n",
        "  \\frac{1}{800} & 0 \\\\ 0 & \\frac{1}{0.048x_2^2} \n",
        "\\end{bmatrix} $ which is a diagonal matrix whose diagonal entries are the inverse second order partial derivatives . "
      ],
      "metadata": {
        "id": "QFznkWdJzxbz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dHzxolp3xiit"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "\n",
        "#method to find Hessian matrix\n",
        "def evalh(x): \n",
        "  assert type(x) is np.ndarray \n",
        "  assert len(x) == 2 \n",
        "  return np.array([[800, 0] , [0,0.048*x[1]**2]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  return 400*x[0]**2 + 0.004*x[1]**4"
      ],
      "metadata": {
        "id": "iaZI8hRP_Lqt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalg(x): #Evaluating gradient of the function\n",
        "  assert type(x) is np.ndarray and len(x) ==2\n",
        "  return np.array([800*x[0],0.016*x[1]**3])"
      ],
      "metadata": {
        "id": "uYgpCivW_4jc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the diagonal matrix for scaling\n",
        "def compute_D_k(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  #compute and return D_k\n",
        "  if np.linalg.det(evalh(x)) == 0:\n",
        "    raise ValueError('Inverse not possible. Please check!')\n",
        "  return np.linalg.inv(evalh(x))"
      ],
      "metadata": {
        "id": "mhfP9-65BAl8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x,gradf,alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(x) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  #assert type(direction) is np.ndarray and len(direction) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0.\n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p = -gradf\n",
        "  D_k = compute_D_k(x)\n",
        "  m = np.matmul(D_k,p)\n",
        "  while evalf(x)<evalf(x+alpha*m)+(np.matmul(np.matrix.transpose(gradf), m))*alpha*gamma:\n",
        "    alpha = alpha*rho\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "mXKDylMw_Ks_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(x) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  #Complete the code \n",
        "  alpha = alpha_start\n",
        "  p = -gradf\n",
        "  #implement the backtracking line search\n",
        "  while evalf(x + alpha*p) > (evalf(x)-(gamma*alpha*np.dot(p,p))):\n",
        "    #while evalf(x + alpha*p) > evalf(x) + gamma * alpha* (np.matmul(np.matrix.transpose(gradf), p) ):  \n",
        "    alpha = rho*alpha\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "38e8ttSIB7wx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BACKTRACKING_LINE_SEARCH = 1\n",
        "CONSTANT_STEP_LENGTH = 2"
      ],
      "metadata": {
        "id": "4c5TioFJF2o-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for gradient descent:"
      ],
      "metadata": {
        "id": "HdNdvv4gKi4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the code for gradient descent without scaling to find the minimizer\n",
        "def find_minimizer_gd(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0. \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  gradf = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "\n",
        "  k = 0\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,gradf, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 1.0\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    gradf = evalg(x)\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x ,k\n"
      ],
      "metadata": {
        "id": "msXJZtFVFb6M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        "\n",
        "def find_minimizer_gdscaling(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0.\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  gradf = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "\n",
        "  k = 0\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    D_k = compute_D_k(x) \n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x,gradf,alpha_start, rho, gamma) #call the new function you wrote to compute the steplength\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 1.0\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(D_k,g_x)))\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    gradf = evalg(x)\n",
        "\n",
        "  return x ,k"
      ],
      "metadata": {
        "id": "eDi46aG8GKv5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que-3:"
      ],
      "metadata": {
        "id": "GplpOa6BMfZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x = np.array([2.,2.])\n",
        "my_tol= 1e-9\n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5"
      ],
      "metadata": {
        "id": "YqFuCuQTMepa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "glji_B9P_ckL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Newton Method with Constant Step Length:\")\n",
        "x_cons, k_cons = find_minimizer_gdscaling(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\n",
        "print(\"\\t\\t The Minimizer is : \",x_cons,\"\\n \\t\\t The minimum objective function value: \",evalf(x_cons),\"\\n \\t\\t And Num. of iterations: \",k_cons)\n",
        "print(\"For Newton Method with Back Tracking Line Search:\")\n",
        "x_bls, k_bls = find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH,alpha_start,rho,gamma)\n",
        "print(\"\\t\\t The Minimizer is : \",x_bls,\"\\n \\t\\t The minimum objective function value: \",evalf(x_bls),\"\\n \\t\\t And Num. of iterations: \",k_bls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEFUA0bEMxUD",
        "outputId": "161c3f28-7a2c-41d5-de4f-67dfd9b3e1cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Newton Method with Constant Step Length:\n",
            "\t\t The Minimizer is :  [0.         0.00304488] \n",
            " \t\t The minimum objective function value:  3.4382653805813626e-13 \n",
            " \t\t And Num. of iterations:  16\n",
            "For Newton Method with Back Tracking Line Search:\n",
            "\t\t The Minimizer is :  [0.         0.00304488] \n",
            " \t\t The minimum objective function value:  3.4382653805813626e-13 \n",
            " \t\t And Num. of iterations:  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the above output that number of iterations for newton's method using both type of method(Constant step length and the backtracking line search) are same i.e. 16 . \n",
        "\n",
        "And the Minimizer and minimum function values are also same. which denotes that for this particular starting point, both of the methods are almost equally efficient under newton's method algorithm."
      ],
      "metadata": {
        "id": "YVWT-TSBTFV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#In the given Function, hessian matrix itself is a diagonal matrix. So, the matrix $D_k$ will be the inverse of the hessian matrix itself."
      ],
      "metadata": {
        "id": "E6k2oLWkW-wE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Que-4:"
      ],
      "metadata": {
        "id": "RyoE6IXRPjxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"For Gradient Descent Method with Constant Step Length(without scaling):\")\n",
        "#x_cons_ws, k_cons_ws = find_minimizer_gd(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\n",
        "#print(\"\\t\\t The Minimizer is : \",x_cons_ws,\"\\n \\t\\t The minimum objective function value: \",evalf(x_cons_ws),\"\\n \\t\\t And Num. of iterations: \",k_cons_ws)\n",
        "print(\"For Gradient Descent Method with Back Tracking Line Search(with scaling and using a diagonal matrix):\")\n",
        "x_bls_sc, k_bls_sc = find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH,alpha_start,rho,gamma)\n",
        "print(\"\\t\\t The Minimizer is : \",x_bls_sc,\"\\n \\t\\t The minimum objective function value: \",evalf(x_bls_sc),\"\\n \\t\\t And Num. of iterations: \",k_bls_sc)\n",
        "print(\"For Gradient Descent Method with Back Tracking Line Search(without scaling):\")\n",
        "x_bls_ws, k_bls_ws = find_minimizer_gd(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH,alpha_start,rho,gamma)\n",
        "print(\"\\t\\t The Minimizer is : \",x_bls_ws,\"\\n \\t\\t The minimum objective function value: \",evalf(x_bls_ws),\"\\n \\t\\t And Num. of iterations: \",k_bls_ws)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "fdgWAD8nPxDb",
        "outputId": "2ae5a382-abed-4319-9896-181ecd584cb0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Gradient Descent Method with Back Tracking Line Search(with scaling and using a diagonal matrix):\n",
            "\t\t The Minimizer is :  [0.         0.00304488] \n",
            " \t\t The minimum objective function value:  3.4382653805813626e-13 \n",
            " \t\t And Num. of iterations:  16\n",
            "For Gradient Descent Method with Back Tracking Line Search(without scaling):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-737bc7299128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\\t The Minimizer is : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bls_sc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n \\t\\t The minimum objective function value: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bls_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n \\t\\t And Num. of iterations: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_bls_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"For Gradient Descent Method with Back Tracking Line Search(without scaling):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mx_bls_ws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_bls_ws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_minimizer_gd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_start_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_tol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\\t The Minimizer is : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bls_ws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n \\t\\t The minimum objective function value: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bls_ws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n \\t\\t And Num. of iterations: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_bls_ws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4b3e3cc840a1>\u001b[0m in \u001b[0;36mfind_minimizer_gd\u001b[0;34m(start_x, tol, line_search_type, *args)\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#continue as long as the norm of gradient is not close to zero upto a tolerance tol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline_search_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mstep_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_steplength_backtracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#call the new function you wrote to compute the steplength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mline_search_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCONSTANT_STEP_LENGTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#do a gradient descent with constant step length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mstep_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d5ed336eb4f6>\u001b[0m in \u001b[0;36mcompute_steplength_backtracking\u001b[0;34m(x, gradf, alpha_start, rho, gamma)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;31m#Complete the code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Gradient descent algorithm, we get the minimum value value of the function in a mere of 16 iterations when we use scaling using the diagonal matrix. But we may get the minimum value of function in a very high time and after running it for several hours in case of using gradient descent algorithm without scaling. That is why I have stopped the code after running it for 3 hrs."
      ],
      "metadata": {
        "id": "0WJXvn2pUPXa"
      }
    }
  ]
}