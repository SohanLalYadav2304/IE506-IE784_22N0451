{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 2. Exercise 1. }$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "Recall that we implemented the gradient descent algorithm to solve $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$. The main ingredients in the gradient descent iterations are the descent direction $\\mathbf{p}^k$ which is set to $-\\nabla f(\\mathbf{x}^k)$, and the step length $\\eta^k$ which is found by solving an optimization problem (or sometimes taken as a constant value over all iterations). We used the following procedure in the previous lab:\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "& \\mathbf{p}^k =-\\nabla f(\\mathbf{x}^k) \\\\ \n",
        "&\\textbf{While } \\| \\mathbf{p}^k \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k + \\eta  \\mathbf{p}^k) = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} = \\mathbf{x}^k + \\eta^k \\mathbf{p}^k = \\mathbf{x}^k - \\eta^k \\nabla f (\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ivDCuJRP9b"
      },
      "source": [
        "We saw that for particular cases of quadratic functions, a closed form analytical solution for the minimizer of the optimization problem $\\min_{\\eta \\geq 0} f({\\mathbf{x}}^k + \\eta {\\mathbf{p}}^k)$ exists. However finding a closed form expression as a solution to this optimization problem to find a suitable step length might not always be possible. To tackle general situations, we will try to devise a different procedure in this lab. \n",
        "\n",
        "To find the step length, we will use the following property: \n",
        "Suppose a non-zero $\\mathbf{p} \\in {\\mathbb{R}}^n$ is a descent direction at point $\\mathbf{x}$, and let $\\gamma \\in (0,1)$. Then there exists $\\varepsilon >0$ such that  \n",
        "\\begin{align}\n",
        "f(\\mathbf{x}+\\alpha \\mathbf{p}) \\leq f(\\mathbf{x}) + \\gamma \\alpha \\nabla f(\\mathbf{x})^\\top \\mathbf{p}, \\ \\forall \\alpha \\in (0,\\varepsilon].  \n",
        "\\end{align}\n",
        "\n",
        "The step length $\\eta^k$ can be found using a backtracking procedure illustrated below to find appropriate value of $\\varepsilon$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW-xcDISWmGR"
      },
      "source": [
        "In this exercise, we will check if finding the steplength using the backtracking procedure is advantageous for some quadratic functions. In this sample code we consider $f(\\mathbf{x})=f(x_1,x_2) = (x_1-8)^2 + (x_2 + 12)^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  return (x[1]+12)**2 + (-8+x[0])**2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  return np.array([2*(x[0]-8), 2*(x[1]+12)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf, A,x): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 2 and  A.shape[1] == 2 #allow only a 2x2 array\n",
        "   \n",
        "  numerator=np.dot(evalg(x).transpose(),evalg(x))    \n",
        "  denominator=2*np.dot(np.dot(evalg(x).transpose(),A),evalg(x))\n",
        "  step_length=numerator/denominator\n",
        "\n",
        "  \n",
        "  return step_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV6OddaNAmpA"
      },
      "source": [
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:}  \\text{ $\\mathbf{x}^k$, $\\mathbf{p}^k$, $\\alpha^0$, $\\rho \\in (0,1)$, $\\gamma \\in (0,1)$ }  \\\\\n",
        "& \\textbf{Initialize } \\alpha=\\alpha^0 \\\\ \n",
        "&\\textbf{While } f(\\mathbf{x}^k + \\alpha \\mathbf{p}^k)   > f(\\mathbf{x}^k) + \\gamma \\alpha \\nabla f(\\mathbf{x}^k)^\\top \\mathbf{p}^k \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\alpha = \\rho \\alpha  \\\\\n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\alpha\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p=-gradf\n",
        "  while evalf(x+alpha*p)> evalf(x)+gamma*alpha*(np.dot(evalg(x).transpose(),p)):\n",
        "    alpha=alpha*rho\n",
        "  return alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad=evalg(np.array([1,1]))"
      ],
      "metadata": {
        "id": "zbFu9nwr4ceM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaUUdzLtVSCl"
      },
      "source": [
        "#we define the types of line search methods that we have implemented\n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1, 0],[0,1]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A,x) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{[R]} \\\\ \\text{Note down the minimizer and minimum function value of }f(x) = f(x_1, x_2) = (x_1 − 8)^2 + (x_2 + 12)^2. $"
      ],
      "metadata": {
        "id": "7icJC48dB5iD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d41c69-6a32-49a0-abaa-720b5f2a450a"
      },
      "source": [
        "\n",
        "my_start_x = np.array([1,1])\n",
        "my_tol= 1e-5\n",
        "\n",
        "print('Solution using constant_step_length(0.1)')\n",
        "x_opt,number_of_iter = find_minimizer(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\n",
        "print('Minimizer of function:',x_opt)\n",
        "print('Minuimum function value:',evalf(x_opt))\n",
        "print('number of iterations:',number_of_iter)\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Solution using closed-form expression')\n",
        "x_opt_cls,number_of_iter_cls=find_minimizer(my_start_x,my_tol,EXACT_LINE_SEARCH)\n",
        "print('Minimizer of function:',x_opt_cls)\n",
        "print('Minuimum function value:',evalf(x_opt_cls))\n",
        "print('number of iterations:',number_of_iter_cls)\n",
        "print('---------------------------------------------------------------------')\n",
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "print('Solution using backtracking line search')\n",
        "x_opt_bls,number_of_iter_bls = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print('Minimizer of function:',x_opt_bls)\n",
        "print('Minuimum function value:',evalf(x_opt_bls))\n",
        "print('number of iterations:',number_of_iter_bls)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution using constant_step_length(0.1)\n",
            "Minimizer of function: [  7.99999775 -11.99999582]\n",
            "Minuimum function value: 2.2517218946096954e-11\n",
            "number of iterations: 67\n",
            "---------------------------------------------------------------------\n",
            "Solution using closed-form expression\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 0.0\n",
            "number of iterations: 1\n",
            "---------------------------------------------------------------------\n",
            "Solution using backtracking line search\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 0.0\n",
            "number of iterations: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{ Above result shows minimizer for function and minimum function value in various ways of finding step length.} $"
      ],
      "metadata": {
        "id": "hidTQuboRqtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{[R]} \\text{Consider stopping tolerance} \\\\ τ = 10^{−12} \\text{and starting point} \\ x_0 = (25, 25). \\\\ \\text{ Compare the number of iterations taken by the gradient descent procedure which uses exact step length computation against the gradient descent} \\\\ \\text{procedure which uses the backtracking line search procedure (with} α_0 = 1, ρ = 0.5, γ = 0.5). \\text{Comment on your observations.} $"
      ],
      "metadata": {
        "id": "O9aX-d9PGTsK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f754a2-20af-4eda-e697-90f23d7d73c3"
      },
      "source": [
        "Start_upd=np.array([25,25])\n",
        "tol_upd=1e-12\n",
        "print('Solution using closed-form expression')\n",
        "x_opt_cls,number_of_iter_cls=find_minimizer(Start_upd,tol_upd,EXACT_LINE_SEARCH)\n",
        "print('Minimizer of function:',x_opt_cls)\n",
        "print('Minuimum function value:',evalf(x_opt_cls))\n",
        "print('number of iterations:',number_of_iter_cls)\n",
        "print('---------------------------------------------------------------------')\n",
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "print('Solution using backtracking line search')\n",
        "x_opt_bls,number_of_iter_bls = find_minimizer(Start_upd,tol_upd, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print('Minimizer of function:',x_opt_bls)\n",
        "print('Minuimum function value:',evalf(x_opt_bls))\n",
        "print('number of iterations:',number_of_iter_bls)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution using closed-form expression\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 0.0\n",
            "number of iterations: 1\n",
            "---------------------------------------------------------------------\n",
            "Solution using backtracking line search\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 0.0\n",
            "number of iterations: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\\\ \\text{As we can see that number of iterations in both ways(exact step length computation,backtracking line search procedure) are equal to 1} \\\\ \\text{ so we can see that in this condition when starting point is [25,25] and tolerance value is 1e-12 } $"
      ],
      "metadata": {
        "id": "njEmrY67SCUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{[R] With starting point} \\ x_0 = (25, 25) \\ \\text{ and} \\ τ = 10^{−10} \\\\ \\text{we will now study the behavior of the backtracking line search algorithm for different choices of} \\  \\alpha_0.\\ \\text{Take} \\ γ = ρ = 0.5. \\\\ \\text{ Try} \\ \\alpha_0 ∈ \\{1, 0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01\\}.$\n",
        "For each α0, record the final minimizer, final objective function value and number of iterations taken by the\n",
        "gradient descent algorithm with backtracking line search to terminate. Prepare a plot where the number of\n",
        "iterations is plotted against α0 values. Comment on the observations. Comment about the minimizers and\n",
        "objective function values obtained for different choices of the α0 values. Check and comment if for any α0\n",
        "value, gradient descent with backtracking line search takes lesser number of iterations when compared to the\n",
        "gradient descent procedure with exact line search."
      ],
      "metadata": {
        "id": "fJN12X_fIelo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Start_upd=np.array([25,25])\n",
        "tol_upd=1e-10\n",
        "alpha_0=[1,0.9,0.75,0.6,0.5,0.4,0.25,0.1,0.01]\n",
        "iter_list=[]\n",
        "print('Solution using closed-form expression')\n",
        "x_opt_cls,number_of_iter_cls=find_minimizer(Start_upd,tol_upd,EXACT_LINE_SEARCH)\n",
        "print('Minimizer of function:',x_opt_cls)\n",
        "print('Minuimum function value:',evalf(x_opt_cls))\n",
        "print('number of iterations:',number_of_iter_cls)\n",
        "print('---------------------------------------------------------------------')\n",
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "print('Solution using backtracking line search')\n",
        "for alpha in alpha_0:\n",
        "  x_opt_bls,number_of_iter_bls = find_minimizer(Start_upd,tol_upd, BACKTRACKING_LINE_SEARCH, alpha, 0.5,0.5)\n",
        "  iter_list.append(number_of_iter_bls)\n",
        "  print('initial value of alpha:',alpha)\n",
        "  print('Minimizer of function:',x_opt_bls)\n",
        "  print('Minuimum function value:',evalf(x_opt_bls))\n",
        "  print('number of iterations:',number_of_iter_bls)\n",
        "  print('---------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQN4CkfuHs-D",
        "outputId": "3cdb7285-4e34-47f6-b6c5-4d856ff76a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution using closed-form expression\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 0.0\n",
            "number of iterations: 1\n",
            "---------------------------------------------------------------------\n",
            "Solution using backtracking line search\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "initial value of alpha: 1\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 0.0\n",
            "number of iterations: 1\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 0.9 rho: 0.5  gamma: 0.5\n",
            "initial value of alpha: 0.9\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 1.6579714975258972e-21\n",
            "number of iterations: 12\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 0.75 rho: 0.5  gamma: 0.5\n",
            "initial value of alpha: 0.75\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 1.3714654556129199e-21\n",
            "number of iterations: 20\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 0.6 rho: 0.5  gamma: 0.5\n",
            "initial value of alpha: 0.6\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 2.2038291998576117e-21\n",
            "number of iterations: 30\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 0.5 rho: 0.5  gamma: 0.5\n",
            "initial value of alpha: 0.5\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 0.0\n",
            "number of iterations: 1\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 0.4 rho: 0.5  gamma: 0.5\n",
            "initial value of alpha: 0.4\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 1.1393259623274523e-22\n",
            "number of iterations: 18\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 0.25 rho: 0.5  gamma: 0.5\n",
            "initial value of alpha: 0.25\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 1.3714654556129199e-21\n",
            "number of iterations: 40\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 0.1 rho: 0.5  gamma: 0.5\n",
            "initial value of alpha: 0.1\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 2.3972320602008796e-21\n",
            "number of iterations: 123\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 0.01 rho: 0.5  gamma: 0.5\n",
            "initial value of alpha: 0.01\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 2.4523367712209537e-21\n",
            "number of iterations: 1358\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(alpha_0,iter_list,color='r')\n",
        "plt.scatter(alpha_0,iter_list,color='g')\n",
        "plt.title('Number of iteration vs alpha_0 value')\n",
        "plt.xlabel('alpha_0 value')\n",
        "plt.ylabel('number of iteration')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "wR6fZ-PZKl7b",
        "outputId": "78c54145-7d74-49e2-9e7b-fa2b8984fe22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\text{ Here we can easily observe that when value of alpha is near to zero than gradient descent takes high number of iterations} \\\\ $ \\\\\n",
        " but as soon as alplha value increases and tends to zero it starts decreasing and at value 0.5 , we get minimum number of iterations which is 1 and as we increase value of alpha more than 0.5 than number of iterations again start increasing and increase slowly thus we can conclude that small alpha value results more number of iterations. "
      ],
      "metadata": {
        "id": "KdQqNgpOMKe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Here we can observe that for alpha =1 and alpha=0.5 we get number of iterations is 1 which is minimum and for all other values of alpha our gradient descent method take more number of iterations to reach optimal condition and when we use exact line search to find step length, it also takes one iteration to reach optimal condition. So there is no values of alpha where we got number of iterations less than number of iterations to reach optial condition in exact line search methoed to find step_length."
      ],
      "metadata": {
        "id": "K0icqwVuUQug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{[R]}$  $ \\text{With starting point} \\ x0 = (25, 25)  \\ \\text{and} \\  τ = 10^{−10} \\ \\text{,we will now study the behavior of the backtracking line search algorithm for different choices of } \\ ρ. \\text{Take} \\ α = 1, γ = 0.5. \\ \\text{Try} \\ $ $ ρ ∈ \\{{0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01\\}}.$ \\\\\n",
        "For each ρ, record the final minimizer, final objective function value and number of iterations taken by the\n",
        "gradient descent algorithm with backtracking line search to terminate. Prepare a plot where the number of\n",
        "iterations is plotted against ρ values. Comment on the observations. Comment about the minimizers and\n",
        "objective function values obtained for different choices of the ρ values. Check and comment if for any ρ value,\n",
        "gradient descent with backtracking line search takes lesser number of iterations when compared to the gradient\n",
        "descent procedure with exact line search."
      ],
      "metadata": {
        "id": "Nfv48wb5MACa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Start_upd=np.array([25,25])\n",
        "tol_upd=1e-10\n",
        "rho_list=[0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01]\n",
        "alpha=1\n",
        "iter_list_1=[]\n",
        "print('Solution using closed-form expression')\n",
        "x_opt_cls,number_of_iter_cls=find_minimizer(Start_upd,tol_upd,EXACT_LINE_SEARCH)\n",
        "print('Minimizer of function:',x_opt_cls)\n",
        "print('Minuimum function value:',evalf(x_opt_cls))\n",
        "print('number of iterations:',number_of_iter_cls)\n",
        "print('---------------------------------------------------------------------')\n",
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "print('Solution using backtracking line search')\n",
        "for rho in rho_list:\n",
        "  x_opt_bls,number_of_iter_bls = find_minimizer(Start_upd,tol_upd, BACKTRACKING_LINE_SEARCH, alpha, rho,0.5)\n",
        "  iter_list_1.append(number_of_iter_bls)\n",
        "  print('initial value of rho:',rho)\n",
        "  print('Minimizer of function:',x_opt_bls)\n",
        "  print('Minuimum function value:',evalf(x_opt_bls))\n",
        "  print('number of iterations:',number_of_iter_bls)\n",
        "  print('---------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfGGpWUsLks3",
        "outputId": "3867c204-0306-468c-b3db-e08d7d95125f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution using closed-form expression\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 0.0\n",
            "number of iterations: 1\n",
            "---------------------------------------------------------------------\n",
            "Solution using backtracking line search\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.9  gamma: 0.5\n",
            "initial value of rho: 0.9\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 4.960536411900771e-22\n",
            "number of iterations: 9\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.75  gamma: 0.5\n",
            "initial value of rho: 0.75\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 1.0819468296335504e-21\n",
            "number of iterations: 15\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.6  gamma: 0.5\n",
            "initial value of rho: 0.6\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 7.844395544174143e-22\n",
            "number of iterations: 22\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "initial value of rho: 0.5\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 0.0\n",
            "number of iterations: 1\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.4  gamma: 0.5\n",
            "initial value of rho: 0.4\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 1.1393259623274523e-22\n",
            "number of iterations: 18\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.25  gamma: 0.5\n",
            "initial value of rho: 0.25\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 1.3714654556129199e-21\n",
            "number of iterations: 40\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.1  gamma: 0.5\n",
            "initial value of rho: 0.1\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 2.3972320602008796e-21\n",
            "number of iterations: 123\n",
            "---------------------------------------------------------------------------\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.01  gamma: 0.5\n",
            "initial value of rho: 0.01\n",
            "Minimizer of function: [  8. -12.]\n",
            "Minuimum function value: 2.4523367712209537e-21\n",
            "number of iterations: 1358\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rho_list,iter_list_1,color='r')\n",
        "plt.scatter(rho_list,iter_list_1,color='b')\n",
        "plt.title('Number of iteration vs rho value')\n",
        "plt.xlabel('rho value')\n",
        "plt.ylabel('number of iteration')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "uncSd_MhQxDm",
        "outputId": "13058a2e-637d-42dd-8c4d-44442ec369d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Explaination of plot}$\n",
        "\n",
        "$\\large $As the above plot explains that when row value goes near to 0 than gradient descent take high number of iterations to reach optiaml condition and as value of rho reaches to 0.5 it take minimum number of itartions(1) to reach optimal conditions."
      ],
      "metadata": {
        "id": "JMgddWuzVIcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Observations about minimizers and minimum function value:}$ \n",
        "\n",
        "As we can see that for each value of rho the minimum function value is zero or so small quantity which is approximately equal to zero. Similarly for each value of rho, minimizers are [8,-12] or almost equal to these values."
      ],
      "metadata": {
        "id": "wm-q4ouYWOEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Comparison of number of iterations based on  methos of finding step_length} $ \n",
        "\n",
        "By above results we know that number of iterations is 1 when we use exact line search method to find step_lenght and if we take different rho values in backtracking line search method we get different number of methods and minimum number of iterations are 1 corresponding to rho 0.5 so there is no value of rho which can give us lower number of iterations to reach optimal conditions in comparison of exact line search method to find step length method."
      ],
      "metadata": {
        "id": "eONMCZFzXGKS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cuB66dSbSmRS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}