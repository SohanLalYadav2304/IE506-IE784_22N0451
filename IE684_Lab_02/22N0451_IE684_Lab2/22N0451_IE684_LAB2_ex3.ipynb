{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "eCz6sJqwIrgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\large \\text{Consider the function: } \\\\ h(x)= 512(x_2-x_1^2)^2+(4-x_1)^2 $"
      ],
      "metadata": {
        "id": "EwJ52irqE3Wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Answer: minimizer of function h(x) is [4,16] and minimum function value is 0 } $"
      ],
      "metadata": {
        "id": "lx_rPEbZG_cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{[R] Can you use the exact line search idea to find a closed form expression for solving minη q(x + ηp) where} \n",
        "\\\\ \\text{p not equal to 0 is a descent direction at x? If yes, find the closed form expression for optimal value of η. If you cannot\n",
        "find closed form solution, explain the reasons.}$"
      ],
      "metadata": {
        "id": "j5kvp7NDHR8R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8ToV_GQEuwj"
      },
      "outputs": [],
      "source": [
        "def evalf(x):\n",
        "  assert len(x)==2 and type(x) is np.ndarray\n",
        "  return 512*(x[1]-x[0]**2)**2 + (4-x[0])**2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evalg(x):\n",
        "  assert len(x)==2  and type(x) is np.ndarray\n",
        "  return np.array([1024*(x[1]-x[0]**2)*(-2*x[0])-2*(4-x[0]),1024*(x[1]-x[0]**2)])"
      ],
      "metadata": {
        "id": "9hhdvJ7XI6wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_exact(x):\n",
        "  y_1,y_2,a_1,a_2,a_3,a_4=[0 for i in range(6)]\n",
        "  y_1,y_2,a_1,a_2,a_3,a_4= [np.longdouble(y_1),np.longdouble(y_2),np.longdouble(a_1),np.longdouble(a_2),np.longdouble(a_3),np.longdouble(a_4)]\n",
        "  y_1=1024*(x[0]*2)-1024*(x[1])\n",
        "  y_2=8-2*x[0]-2048*(x[0]**3)+2048*x[0]*x[1]\n",
        "  a_1=(2048*(y_2*4))/(10*8)\n",
        "  a_2=(1024*(y_2*2)*(y_1-2*x[0]*y_2) + 2048*(y_2*2)*(y_1-2*x[0]*y_2))/(10*8)\n",
        "  a_2=-a_2\n",
        "  a_3=(1024*(y_1-2*x[0]*y_2)*(y_1-2*x[0]*y_2)-2048*(y_2*2)*(x[1]-(x[0]**2)) + 2*(y_2*2))/(10*8)\n",
        "  a_4=(1024*(y_1-2*x[0]*y_2)*(x[1]-(x[0]**2)) -2*y_2*(4-x[0]))/(10**8)\n",
        "  root=np.roots([a_1,a_2,a_3,a_4])\n",
        "  for i in root:\n",
        "    if 3*a_1*(i**2) + 2*a_2*i +a_3 > 0:\n",
        "      return i"
      ],
      "metadata": {
        "id": "_IDN9xc-ADRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{we obtained step length value of exact line search expression value of alpha so we are able to do it with exact line search method.} $"
      ],
      "metadata": {
        "id": "ahfp-wknIGo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p=-gradf\n",
        "  while evalf(x+alpha*p)> evalf(x)+gamma*alpha*(np.dot(evalg(x).transpose(),p)):\n",
        "    alpha=alpha*rho\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "ZJt8cDbNJpyt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2"
      ],
      "metadata": {
        "id": "FuCQAiE6B1UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\large \\text{[R] With starting point x0 = (100, 100) and τ = 10−10, we will now study the behavior of the backtracking line} \\\\ \\text{\n",
        "search algorithm for different choices of α0.Take} γ = ρ = 0.5. \\text{Try α0 ∈ {1, 0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01}.} \\\\ \n",
        "\\text{For each α0, record the final minimizer, final objective function value and number of iterations taken by the} \\\\ \\text{\n",
        "gradient descent algorithm with backtracking line search to terminate. Prepare a plot where the number of iterations is plotted against α0 values. Comment on the observations. Comment about the minimizers and} \\\\ $\n"
      ],
      "metadata": {
        "id": "TXY-oBrMIwH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\text{\n",
        "objective function values obtained for different choices of the α0 values. If you have implemented exact line } $\n",
        "search, check and comment if for any α0 value, gradient descent with backtracking line search takes lesser\n",
        "number of iterations when compared to the gradient descent procedure with exact line search.$ "
      ],
      "metadata": {
        "id": "C8GvffN1Kggx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{I will bound the number of iterations by 10000} $"
      ],
      "metadata": {
        "id": "Amq95XoxMQRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_cls(start_x, tol):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  \n",
        "\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength_exact(x) #call the new function you wrote to compute the steplength\n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    if k>10000:\n",
        "      break\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k "
      ],
      "metadata": {
        "id": "1p1N9bUoU98h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_bls(start_x, tol, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  alpha_start = args[0]\n",
        "  rho = args[1]\n",
        "  gamma = args[2]\n",
        "  print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    if k>10000:\n",
        "      break\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k "
      ],
      "metadata": {
        "id": "8Ty8mrSwU8YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start=np.array([100,100])\n",
        "tol=1e-10\n",
        "\n",
        "print('Solution using closed-form expression')\n",
        "x_opt_cls,number_of_iter_cls=find_minimizer_cls(start,tol)\n",
        "print('Minimizer of function:',x_opt_cls)\n",
        "print('Minuimum function value:',evalf(x_opt_cls))\n",
        "print('number of iterations:',number_of_iter_cls)\n",
        "print('---------------------------------------------------------------------')\n",
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "print('Solution using backtracking line search')\n",
        "alpha_list=np.array([1,0.9,0.75,0.6,0.5,0.4,0.25,0.1,0.01])\n",
        "for alpha in alpha_list:\n",
        "  start=np.array([100,100])\n",
        "  tol=1e-10\n",
        "  x_opt_bls,number_of_iter_bls = find_minimizer_bls(start, tol, alpha, 0.5,0.5)\n",
        "  print('Minimizer of function:',x_opt_bls)\n",
        "  print('Minuimum function value:',evalf(x_opt_bls))\n",
        "  print('number of iterations:',number_of_iter_bls)\n",
        "  print('\\n---------------------------------------------------------\\n')"
      ],
      "metadata": {
        "id": "hq2Be26xLf_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7be979-9bbb-4077-ba1c-5546ec8d9d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution using closed-form expression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-8c4bed1c1b82>:7: RuntimeWarning: overflow encountered in long_scalars\n",
            "  a_2=(1024*(y_2*2)*(y_1-2*x[0]*y_2) + 2048*(y_2*2)*(y_1-2*x[0]*y_2))/(10*8)\n",
            "<ipython-input-12-8c4bed1c1b82>:9: RuntimeWarning: overflow encountered in long_scalars\n",
            "  a_3=(1024*(y_1-2*x[0]*y_2)*(y_1-2*x[0]*y_2)-2048*(y_2*2)*(x[1]-(x[0]**2)) + 2*(y_2*2))/(10*8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer of function: [ 2.21496135e+09 -1.10747052e+07]\n",
            "Minuimum function value: 1.2323514115394172e+40\n",
            "number of iterations: 10001\n",
            "---------------------------------------------------------------------\n",
            "Solution using backtracking line search\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [ 10.0476072  100.95498684]\n",
            "Minuimum function value: 36.57372294589531\n",
            "number of iterations: 10001\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Params for Backtracking LS: alpha start: 0.9 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [ 10.04766087 100.95613638]\n",
            "Minuimum function value: 36.57441661039108\n",
            "number of iterations: 10001\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Params for Backtracking LS: alpha start: 0.75 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [ 10.04637453 100.93015577]\n",
            "Minuimum function value: 36.55878053071092\n",
            "number of iterations: 10001\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Params for Backtracking LS: alpha start: 0.6 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [ 10.02561479 100.5135538 ]\n",
            "Minuimum function value: 36.30821907442939\n",
            "number of iterations: 10001\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Params for Backtracking LS: alpha start: 0.5 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [ 10.0476072  100.95498684]\n",
            "Minuimum function value: 36.57372294589531\n",
            "number of iterations: 10001\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Params for Backtracking LS: alpha start: 0.4 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [ 10.04686477 100.94000085]\n",
            "Minuimum function value: 36.56470626545084\n",
            "number of iterations: 10001\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Params for Backtracking LS: alpha start: 0.25 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [ 10.0476072  100.95498684]\n",
            "Minuimum function value: 36.57372294589531\n",
            "number of iterations: 10001\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Params for Backtracking LS: alpha start: 0.1 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [ 10.04686477 100.94000085]\n",
            "Minuimum function value: 36.56470626545084\n",
            "number of iterations: 10001\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Params for Backtracking LS: alpha start: 0.01 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [ 9.92534351 98.51301268]\n",
            "Minuimum function value: 35.109861410148916\n",
            "number of iterations: 10001\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. [R] With starting point x0 = (100, 100) and τ = 10−10, we will now study the behavior of the backtracking line\n",
        "search algorithm for different choices of ρ. Take α = 1, γ = 0.5. Try ρ ∈ {0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01}.\n",
        "For each ρ, record the final minimizer, final objective function value and number of iterations taken by the\n",
        "gradient descent algorithm with backtracking line search to terminate. Prepare a plot where the number of\n",
        "iterations is plotted against ρ values. Comment on the observations. Comment about the minimizers and\n",
        "objective function values obtained for different choices of the ρ values. If you have implemented exact line\n",
        "search, check and comment if for any ρ value, gradient descent with backtracking line search takes lesser\n",
        "number of iterations when compared to the gradient descent procedure with exact line search."
      ],
      "metadata": {
        "id": "u_LL1xrqM4UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Solution using closed-form expression')\n",
        "x_opt_cls,number_of_iter_cls=find_minimizer_cls(start,tol)\n",
        "print('Minimizer of function:',x_opt_cls)\n",
        "print('Minuimum function value:',evalf(x_opt_cls))\n",
        "print('number of iterations:',number_of_iter_cls)\n",
        "print('---------------------------------------------------------------------')\n",
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "print('Solution using backtracking line search')\n",
        "alpha_list=np.array([0.9,0.75,0.6,0.5,0.4,0.25,0.1,0.01])\n",
        "for alpha in alpha_list:\n",
        "  start=np.array([100,100])\n",
        "  tol=1e-10\n",
        "  x_opt_bls,number_of_iter_bls = find_minimizer_bls(start, tol, 1, alpha,0.5)\n",
        "  print('Minimizer of function:',x_opt_bls)\n",
        "  print('Minuimum function value:',evalf(x_opt_bls))\n",
        "  print('number of iterations:',number_of_iter_bls)\n",
        "  print('\\n---------------------------------------------------------\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "Zu08TpgZE2DC",
        "outputId": "7fffd095-1b67-4104-c0af-80f033fc0f8e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution using closed-form expression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-8c4bed1c1b82>:7: RuntimeWarning: overflow encountered in long_scalars\n",
            "  a_2=(1024*(y_2*2)*(y_1-2*x[0]*y_2) + 2048*(y_2*2)*(y_1-2*x[0]*y_2))/(10*8)\n",
            "<ipython-input-12-8c4bed1c1b82>:9: RuntimeWarning: overflow encountered in long_scalars\n",
            "  a_3=(1024*(y_1-2*x[0]*y_2)*(y_1-2*x[0]*y_2)-2048*(y_2*2)*(x[1]-(x[0]**2)) + 2*(y_2*2))/(10*8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer of function: [ 2.21496135e+09 -1.10747052e+07]\n",
            "Minuimum function value: 1.2323514115394172e+40\n",
            "number of iterations: 10001\n",
            "---------------------------------------------------------------------\n",
            "Solution using backtracking line search\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.9  gamma: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-78b251d24ac9>:3: RuntimeWarning: overflow encountered in long_scalars\n",
            "  return 512*(x[1]-x[0]**2)**2 + (4-x[0])**2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-a7feae5300d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mx_opt_bls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_of_iter_bls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_minimizer_bls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minimizer of function:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_opt_bls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minuimum function value:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_opt_bls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-5ae21cd35554>\u001b[0m in \u001b[0;36mfind_minimizer_bls\u001b[0;34m(start_x, tol, *args)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#continue as long as the norm of gradient is not close to zero upto a tolerance tol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mstep_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_steplength_backtracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#call the new function you wrote to compute the steplength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m#implement the gradient descent steps here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#update x = x - step_length*g_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-b854abead446>\u001b[0m in \u001b[0;36mcompute_steplength_backtracking\u001b[0;34m(x, gradf, alpha_start, rho, gamma)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YjZz-J0o7lNK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}