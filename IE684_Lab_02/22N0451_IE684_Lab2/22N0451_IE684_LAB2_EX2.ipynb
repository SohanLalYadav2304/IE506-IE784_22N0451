{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dLftftRf77oo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Consider the function:} h(x)=∑_{i=1}^N \\frac{(x_i-2^i)^2}{8^i} $"
      ],
      "metadata": {
        "id": "zsbs4kYN1uJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Let N =n so:} \\\\ h(x)=∑_{i=1}^n \\frac{(x_i-2^i)^2}{8^i} \\\\ ⇒h(x)=∑_{i=1}^n (\\frac{x_i^2}{8^i}+\\frac{(-x_i)}{2^{2i-1}}+\\frac{1}{2^i}) \\\\ \\text{We can write above function in form of } \\ x^ΤAx+2b^Τx+c \\\\ \\text{where} \\ x= \\begin{bmatrix} \n",
        "\tx_1 & x_2 & x_3 .... & x_n \\\\\n",
        "\t\\end{bmatrix}^Τ \\\\ A= \\text{Diagonal matrix with diagonal entries }\\ \\{ {\\frac{1}{2^3}},\\frac{1}{2^6},\\frac{1}{2^9},.,.,.,,.,\\frac{1}{2^{3*n}} \\} \\\\ b=\\begin{bmatrix} \n",
        "\t\\frac{-1}{2^1} & \\frac{-1}{2^3} & \\frac{-1}{2^5} .... & \\frac{-1}{2^{2*n-1}} \\\\\n",
        "\t\\end{bmatrix} \\\\ c= ∑_{i=1}^n\\frac{1}{2^i}$"
      ],
      "metadata": {
        "id": "dVt-R84i2dom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ So for N=3  \\\\ h(x)=∑_{i=1}^3 (\\frac{x_i^2}{8^i}+\\frac{(-x_i)}{2^{2i-1}}+\\frac{1}{2^i})  $"
      ],
      "metadata": {
        "id": "ZwgdAIZx68kY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DaLLjBQu1MyK"
      },
      "outputs": [],
      "source": [
        "def evalf(x):\n",
        "  assert len(x)==3 and type(x) is np.ndarray\n",
        "  return sum((x[i]-2**(i+1))**2/(8**(i+1)) for i in range(3) )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evalg(x):\n",
        "  assert len(x)==3 and type(x) is np.ndarray\n",
        "  return np.array([(x[i-1])/(2**(3*i-1))-1/(2**(2*i-1)) for i in range(1,4)])"
      ],
      "metadata": {
        "id": "AJpYtiCx9HFR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_exact(gradf, A,x): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 3 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 3 and  A.shape[1] == 3 #allow only a 2x2 array\n",
        "   \n",
        "  numerator=np.dot(evalg(x).transpose(),evalg(x))    \n",
        "  denominator=2*np.dot(np.dot(evalg(x).transpose(),A),evalg(x))\n",
        "  step_length=numerator/denominator\n",
        "\n",
        "  \n",
        "  return step_length"
      ],
      "metadata": {
        "id": "BN3hWHAS-Lfq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 3 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 3 \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p=-gradf\n",
        "  while evalf(x+alpha*p)> evalf(x)+gamma*alpha*(np.dot(evalg(x).transpose(),p)):\n",
        "    alpha=alpha*rho\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "cKjRbZ4r-nO2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we define the types of line search methods that we have implemented\n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2"
      ],
      "metadata": {
        "id": "rWuc7ed7-rfm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 3 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1/8, 0,0],[0,1/64,0],[0,0,1/512]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A,x) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k \n"
      ],
      "metadata": {
        "id": "qQSwW2mi-yOP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{[R] Note down the minimizer and minimum function value of h(x).} $"
      ],
      "metadata": {
        "id": "8IyV5gra__3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Let out starting position is [1,1,1] and tolerance value is 10^(-5)')\n",
        "my_start_x = np.array([1,1,1])\n",
        "my_tol= 1e-5\n",
        "print('Solution using closed-form expression')\n",
        "x_opt_cls,number_of_iter_cls=find_minimizer(my_start_x,my_tol,EXACT_LINE_SEARCH)\n",
        "print('Minimizer of function:',x_opt_cls)\n",
        "print('Minuimum function value:',evalf(x_opt_cls))\n",
        "print('number of iterations:',number_of_iter_cls)\n",
        "print('---------------------------------------------------------------------')\n",
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "print('Solution using backtracking line search')\n",
        "x_opt_bls,number_of_iter_bls = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print('Minimizer of function:',x_opt_bls)\n",
        "print('Minuimum function value:',evalf(x_opt_bls))\n",
        "print('number of iterations:',number_of_iter_bls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twk_opUg_QCM",
        "outputId": "c536b6cc-d69a-4de5-a3f5-b3aac5cda076"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let out starting position is [1,1,1] and tolerance value is 10^(-5)\n",
            "Solution using closed-form expression\n",
            "Minimizer of function: [2.00001649 4.         7.99768767]\n",
            "Minuimum function value: 1.0477122031231117e-08\n",
            "number of iterations: 151\n",
            "---------------------------------------------------------------------\n",
            "Solution using backtracking line search\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [2.         4.         7.99744063]\n",
            "Minuimum function value: 1.2793697352715871e-08\n",
            "number of iterations: 2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Answer:The minimizer of function for N=3 is :[2.00001649,4,7.99768767]} ≃ \\text{[2,4,8] } \\\\ $ "
      ],
      "metadata": {
        "id": "4ME794RZxYdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\large \\text{[R] Consider stopping tolerance} \\ τ = 10^{−10} \\ \\text{and starting point} \\ x_0 = ( 1\n",
        "64 , 1\n",
        "8 , 1). \\\\ \\text{ Compare the number of iterations\n",
        "taken by the gradient descent procedure which uses exact step length computation against}  \\\\ \\text{ the gradient descent\n",
        "procedure which uses the backtracking line search procedure (with} α_0 = 1, ρ = 0.5, γ = 0.5). \\\\ \\text{Comment on your\n",
        "observations.} $"
      ],
      "metadata": {
        "id": "aR1fNXQMAFVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_upd=np.array([1/64,1/8,1])\n",
        "tol_upd=1e-10\n",
        "alpha_0=1\n",
        "rho=0.5\n",
        "gamma=0.5\n",
        "print('Solution using closed-form expression')\n",
        "x_opt_cls,number_of_iter_cls=find_minimizer(start_upd,tol_upd,EXACT_LINE_SEARCH)\n",
        "print('Minimizer of function:',x_opt_cls)\n",
        "print('Minuimum function value:',evalf(x_opt_cls))\n",
        "print('number of iterations:',number_of_iter_cls)\n",
        "print('---------------------------------------------------------------------')\n",
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "print('Solution using backtracking line search')\n",
        "x_opt_bls,number_of_iter_bls = find_minimizer(start_upd,tol_upd, BACKTRACKING_LINE_SEARCH, alpha_0, rho,gamma)\n",
        "print('Minimizer of function:',x_opt_bls)\n",
        "print('Minuimum function value:',evalf(x_opt_bls))\n",
        "print('number of iterations:',number_of_iter_bls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lybkREpv_jzR",
        "outputId": "9ced6730-c737-42be-cf30-5160d0520b5e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution using closed-form expression\n",
            "Minimizer of function: [2.         4.         7.99999998]\n",
            "Minuimum function value: 9.150071377581033e-19\n",
            "number of iterations: 269\n",
            "---------------------------------------------------------------------\n",
            "Solution using backtracking line search\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [2.         4.         7.99999997]\n",
            "Minuimum function value: 1.2748574165464873e-18\n",
            "number of iterations: 4964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Observations:} \\\\ \\text{As we can see than number of iteration taken by the exact line search method is 269 and number of iterations taken by the backtracking} \\\\ \\text{  line search method is 4964 which is far greater than iterations taken by exact line search.As we can conclude than exact line search method} \\\\ \\text{  takes low number of iterations to reach optimal condition.}$"
      ],
      "metadata": {
        "id": "DHOJp5Lvy8KI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\large \\text{[R] Check if similar observations hold in the N = 4 case as well. Choose a starting point} \\ x_0 = ( 1\n",
        "512 , 1\n",
        "64 , 1\n",
        "8 , 1)\n",
        " \\\\ \\text{and for the backtracking line search procedure, use} \\  α_0 = 1, ρ = 0.5, γ = 0.5. \\\\ \\text{Comment on your observations.} $"
      ],
      "metadata": {
        "id": "4ASrs-J_BZSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ So for N=4  \\\\ h(x)=∑_{i=1}^4 (\\frac{x_i^2}{8^i}+\\frac{(-x_i)}{2^{2i-1}}+\\frac{1}{2^i})  $"
      ],
      "metadata": {
        "id": "hQyFmgC3BsIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evalf_n4(x):\n",
        "  assert len(x)==4 and type(x) is np.ndarray\n",
        "  return sum((x[i]-2**(i+1))**2/(8**(i+1)) for i in range(4) )"
      ],
      "metadata": {
        "id": "vnKAR79LBOy5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalg_n4(x):\n",
        "  assert len(x)==4 and type(x) is np.ndarray\n",
        "  return np.array([(x[i-1])/(2**(3*i-1))-1/(2**(2*i-1)) for i in range(1,5)])"
      ],
      "metadata": {
        "id": "BBXogZynB8CX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_exact_n4(gradf, A,x): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 4 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 4 and  A.shape[1] == 4 #allow only a 2x2 array\n",
        "   \n",
        "  numerator=np.dot(evalg_n4(x).transpose(),evalg_n4(x))    \n",
        "  denominator=2*np.dot(np.dot(evalg_n4(x).transpose(),A),evalg_n4(x))\n",
        "  step_length=numerator/denominator\n",
        "\n",
        "  \n",
        "  return step_length"
      ],
      "metadata": {
        "id": "VkEjLnTECDm8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\n",
        "def compute_steplength_backtracking_n4(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 4 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 4 \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  p=-gradf\n",
        "  while evalf_n4(x+alpha*p)> evalf_n4(x)+gamma*alpha*(np.dot(evalg_n4(x).transpose(),p)):\n",
        "    alpha=alpha*rho\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "MlmcjZfeCNlZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_n4(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 4 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  A = np.array([[1/8, 0,0,0],[0,1/64,0,0],[0,0,1/512,0],[0,0,0,1/2**12]])\n",
        "  x = start_x\n",
        "  g_x = evalg_n4(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact_n4(g_x, A,x) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_n4(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg_n4(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k \n"
      ],
      "metadata": {
        "id": "QELAX2RRCUCU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start=np.array([1/512,1/64,1/8,1])\n",
        "tol=1e-10\n",
        "alpha_0=1\n",
        "rho=0.5\n",
        "gamma=0.5\n",
        "print('Solution using closed-form expression')\n",
        "x_opt_cls,number_of_iter_cls=find_minimizer_n4(start,tol,EXACT_LINE_SEARCH)\n",
        "print('Minimizer of function:',x_opt_cls)\n",
        "print('Minuimum function value:',evalf_n4(x_opt_cls))\n",
        "print('number of iterations:',number_of_iter_cls)\n",
        "print('---------------------------------------------------------------------')\n",
        "print('Solution using backtracking line search')\n",
        "x_opt_bls,number_of_iter_bls = find_minimizer_n4(start,tol, BACKTRACKING_LINE_SEARCH, alpha_0, rho,gamma)\n",
        "print('Minimizer of function:',x_opt_bls)\n",
        "print('Minuimum function value:',evalf_n4(x_opt_bls))\n",
        "print('number of iterations:',number_of_iter_bls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH32V---CsDU",
        "outputId": "e759b3f3-23de-4ab3-bc9d-bf53e55ee380"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solution using closed-form expression\n",
            "Minimizer of function: [ 2.          4.          8.         15.99999981]\n",
            "Minuimum function value: 8.8565993523583e-18\n",
            "number of iterations: 2013\n",
            "---------------------------------------------------------------------\n",
            "Solution using backtracking line search\n",
            "Params for Backtracking LS: alpha start: 1 rho: 0.5  gamma: 0.5\n",
            "Minimizer of function: [ 2.         4.         8.        15.9999998]\n",
            "Minuimum function value: 1.0237544252113035e-17\n",
            "number of iterations: 37079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Observations:} \\\\ \\text{As per the above results we came to know that minimizer of function obtained by both methods} \\\\ \\text{  (backtracking-line-search or exact-line-search) is [2,4,8,16] and minimum function value is:} 8.8565993523583e-18 ≃ 0 \\\\ \\text{As we can here also observe that number of iteration taken to reach optimal condition by exact line search method is less than} \\\\ \\text{ number of iterations taken to reach optimal condition by backtracking line search method.Since:} \\\\ 741<13507 \\\\ \\text{number of iterations by exact line search} < \\text{number of iterations by backtracking line search}$"
      ],
      "metadata": {
        "id": "MME2X00XzzD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Question[R]:} \\\\ \\text{Can you also comment on the possible observations for N > 4 case as well, without actually running the\n",
        "program?} $"
      ],
      "metadata": {
        "id": "n06T7JT419BO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\text{Answer :} \\\\ \\text{As we can see that minimizers for values of N=3 and N=4 we obtained the following results:} \\\\ \\text{for } N=3: \\\\ \\text{minimizer is:} = [2,4,8]=[2^1,2^2,2^3] \\\\ \\text{minimum function value is: } = 0 \\\\ \\text{for } N=4: \\\\ \\text{minimizer is:} = [2,4,8,16]=[2^1,2^2,2^3,2^4] \\\\ \\text{minimum function value is: } = 0 \\\\ \\text{Hence we can define this for general value of n also by induction :} \\\\ \\text{for } N=n: \\\\ \\text{minimizer is:} = [2,4,8,....,2^n]=[2^1,2^2,2^3,....,2^n] \\\\ \\text{minimum function value is: } = 0 \\\\ \\text{We can also observe that exact line search method to compute step lenght was taking low number of iterations to reach } \\\\ \\text{ optimal condition in comparison of backtracking line search method.}$"
      ],
      "metadata": {
        "id": "wLqqfVfq2IMq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y3wLt1fwDnhM"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}