{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 3.}$ $\\large\\textbf{Exercise 1.}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "In the last lab, when we tried to solve certain problems of the form $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$ using gradient descent algorithm, we noticed that the algorithm needed a large number of iterations to find the minimizer. Today we will discuss some remedy measures for this issue.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Meohokl4xP"
      },
      "source": [
        "Consider the problem $\\min_{\\mathbf{x}} f(\\mathbf{x}) = 1500x_1^2 + 4x_1 x_2 +  x_2^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvWjvAgnXxS3"
      },
      "source": [
        "Note that the function $f(\\mathbf{x})$ is twice continuously differentiable. First let us investigate the Hessian $\\nabla^2 f(\\mathbf{x})$ of the function. \n",
        "\n",
        "Note that the Hessian $\\nabla^2 f(\\mathbf{x})$ of the function $f(\\mathbf{x})$ is positive definite. \n",
        "\n",
        "Due to the positive definite nature of the Hessian, we shall find the condition number of the Hessian given by $\\kappa\\left (\\nabla^2 f(\\mathbf{x}) \\right ) = \\frac{\\lambda_{\\max} \\left (\\nabla^2 f(\\mathbf{x}) \\right )}{\\lambda_{\\min} \\left (\\nabla^2 f(\\mathbf{x}) \\right )}$, where $\\lambda_{\\max}(\\mathbf{A})$ denotes the maximum eigen value of matrix $\\mathbf{A}$ and $\\lambda_{\\min}(\\mathbf{A})$ denotes the minimum eigen value of matrix $\\mathbf{A}$.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLfdgrmxANif"
      },
      "source": [
        "$\\textbf{Question:}$ Write code to find the Hessian matrix of the function $f(\\mathbf{x}) = 1500x_1^2 + 4x_1 x_2 +  x_2^2$ and its condition number. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Question 1 [R]:} \\text{Consider the function} \\\\  f(x) = 1500x_1^2+ 4x_1x_2 + x_2^2 \\\\\n",
        "\\text{Write code to find the Hessian matrix of f and its\n",
        "condition number.}$"
      ],
      "metadata": {
        "id": "wlULK9F-I8GA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72qwiJ0CDtOX"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "#method to find Hessian matrix: Complete the code\n",
        "def evalh(x): \n",
        "  assert type(x) is np.ndarray \n",
        "  assert len(x) == 2\n",
        "  a=np.array([3000,4,4,2]).reshape(2,2)\n",
        "  return a\n",
        "\n",
        "#method to find the condition number of any square matrix: : Complete the code\n",
        "def find_condition_number(A):\n",
        "  assert type(A) is np.ndarray\n",
        "  assert A.shape[0] == A.shape[1]\n",
        "  eigen_values=np.linalg.eig(A)[0]\n",
        "  return max(eigen_values)/min(eigen_values)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ivDCuJRP9b"
      },
      "source": [
        "The condition number of the Hessian plays a major role in the progress of the iterates of gradient descent towards the optimal solution point. Typically a large value of the condition number indicates that the problem is $\\textbf{ill-conditioned}$ and hence leads to slow progress of the iterates towards the optimal solution point. Now we shall discuss a method which would help in better $\\textbf{conditioning}$ of the problem and hence would help in speeding up the progress of the iterates towards the optimal solution point. \n",
        "\n",
        "Let us first illustrate an equivalent transformation of the problem $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$. Consider the transformation $\\mathbf{x}=\\mathbf{My}$ where $\\mathbf{M}\\in {\\mathbb{R}}^{n \\times n}$ is an invertible matrix and $\\mathbf{y} \\in {\\mathbb{R}}^n$ and consider the equivalent problem $\\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} g(\\mathbf{y}) \\equiv \\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} f(\\mathbf{My})$. \n",
        "\n",
        "$\\textbf{Check:}$ Why are the two problems $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$ and $\\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} g(\\mathbf{y})$  equivalent? \n",
        "\n",
        "Note that the gradient $\\nabla_{\\mathbf{y}} g(\\mathbf{y}) = \\mathbf{M}^\\top \\nabla_{\\mathbf{x}} f(\\mathbf{x})$ and the Hessian is $\\nabla^2_{\\mathbf{y}} g(\\mathbf{y}) = \\mathbf{M}^\\top \\nabla^2_{\\mathbf{x}} f(\\mathbf{x}) \\mathbf{M}$. \n",
        "\n",
        "Hence the gradient descent update to solve $\\min_{\\mathbf{y} \\in {\\mathbb{R}}^n} g(\\mathbf{y})$ becomes: \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "{\\mathbf{y}}^{k+1} &= {\\mathbf{y}}^{k} - \\eta \\nabla_{\\mathbf{y}} g({\\mathbf{y}}^{k}) \\\\\n",
        "\\end{align}\n",
        "\n",
        "Pre-multiplying by $\\mathbf{M}$, we have:\n",
        "\\begin{align}\n",
        "{\\mathbf{M}\\mathbf{y}}^{k+1} &= {\\mathbf{M}\\mathbf{y}}^{k} -  \\eta \\mathbf{M} \\nabla_{\\mathbf{y}} g({\\mathbf{y}}^{k})  \\\\\n",
        "\\implies \\mathbf{x}^{k+1} &= \\mathbf{x}^{k} - \\eta \\mathbf{MM}^\\top \\nabla_{\\mathbf{x}} f({\\mathbf{x}}^{k}) \n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Letting $\\mathbf{D} = \\mathbf{MM}^\\top$, we see that the update is of the form:\n",
        "\\begin{align}\n",
        "\\mathbf{x}^{k+1} &= \\mathbf{x}^{k} - \\eta \\mathbf{D} \\nabla f({\\mathbf{x}}^{k}) \n",
        "\\end{align}\n",
        "\n",
        "Note that the matrix $\\mathbf{D}$ is symmetric and positive definite and hence can be written as $\\mathbf{D} = \\mathbf{B}^2$, where $\\mathbf{B}$ is also symmetric and positive definite. Denoting $\\mathbf{B}= \\mathbf{D}^{\\frac{1}{2}}$, we see that a useful choice for the matrix $\\mathbf{M}$ is $\\mathbf{M} = \\mathbf{B} = \\mathbf{D}^{\\frac{1}{2}}$. \n",
        "\n",
        "The matrix $\\mathbf{D}$ is called a $\\textbf{scaling}$ matrix and helps in scaling the Hessian. We will consider $\\mathbf{D}$ to be a diagonal matrix. Thus it would be useful to find a suitable candidate of the scaling matrix at each iteration which could help in significant progress of the iterates towards the optimal solution. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgHGXcTYNXuw"
      },
      "source": [
        "This discussion leads to the following algorithm:\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "& \\mathbf{p}^k =-\\nabla f(\\mathbf{x}^k) \\\\ \n",
        "&\\textbf{While } \\| \\mathbf{p}^k \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\text{ Choose a suitable scaling matrix }\\mathbf{D}^k. \\\\ \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k + \\eta  \\mathbf{D}^k \\mathbf{p}^k) = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\mathbf{D}^k \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} = \\mathbf{x}^k + \\eta^k \\mathbf{D}^k \\mathbf{p}^k = \\mathbf{x}^k - \\eta^k  \\mathbf{D}^k \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XvbzQ5LeJ_N"
      },
      "source": [
        "Note that the update step in the modified gradient descent scheme uses a scaled gradient. Thus it becomes important to set up some criteria for choosing the $\\mathbf{D}^k$ matrix in every iteration. In this exercise, we will assume $\\mathbf{D}^k$ to be a diagonal matrix. The following questions will help in designing a suitable $\\mathbf{D}^k$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Question.2[R]} $"
      ],
      "metadata": {
        "id": "xGEsaho_JeKJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC5h9vGOfLcr"
      },
      "source": [
        "$\\textbf{Question:}$ Based on our discussion on condition number and the derivation of the gradient descent scheme with scaling, can you identify and write down the matrix $\\mathbf{Q}$ whose condition number needs to be analyzed in the new gradient scheme with scaling? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL8skZZIf8yd"
      },
      "source": [
        "$\\textbf{ Answer:} \\  \\text{Since we can write our function in form of } x^{T}Qx+b^{T}x \\ \\text{.So condition number of Q must be analyzed inn the new gradient schem with scalling.} \\\\ $\n",
        "\n",
        "\n",
        "$Q=\n",
        "\\begin{bmatrix}\n",
        "1500 & 2\\\\\n",
        "2 & 1\n",
        "\\end{bmatrix}$ \n",
        "\n",
        "$\\\\ \\\\ \\\\ \\text{In general we can write our Q matrix as product of a diagonal matrix and hessian metrix of function.As folllows: }  \\\\ \\mathbf{Q}  = (\\mathbf{D}^k)^{\\frac{1}{2}} \\mathbf{H}^k (\\mathbf{D}^k)^{\\frac{1}{2}}\\ \\ \\ \\\\ \\text{Where } \\mathbf{D} \\ \\text{is a diagonal matrix with diagonal elements as reciprocal of eigen values of Hessian matrix and H is hessian matrix.} $"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Question.3[R]}$"
      ],
      "metadata": {
        "id": "4cisp0gpJuTa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrbWd2XigB2N"
      },
      "source": [
        "$\\textbf{Question:}$ Based on the matrix $\\mathbf{Q}$, can you come up with a useful choice for $\\mathbf{D}^k$ (assuming $\\mathbf{D}^k$ to be diagonal)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTIH3LnUgRh6"
      },
      "source": [
        "\n",
        "$\\textbf{Your Answer:}$\n",
        "\n",
        "yes we can take $D^k$ as diag  $[(\\frac{\\delta^2f}{\\delta x_1^2})^{-1},(\\frac{\\delta^2f}{\\delta x_2^2})^{-1} , (\\frac{\\delta^2f}{\\delta x_3^2})^{-1}, ...............(\\frac{\\delta^2f}{\\delta x_n^2})^{-1}] \\\\ i.e. \\text{Each diagonal value of D is resiprocal of  corresponding diagonal entry of hessian matrix. }$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDNH0zgxgaPR"
      },
      "source": [
        "Write code to find the matrix $\\mathbf{D}^k$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#The method defines a way to construct D_k matrix used in scaling the gradient in the modified gradient descent scheme\n",
        "def compute_D_k(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  Hes=evalh(x)\n",
        "  D_k=np.array([1/Hes[0,0],0,0,1/Hes[1,1]]).reshape(2,2)\n",
        "  return D_k\n",
        "  \n",
        "  "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a Python function which will compute and return the objective function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  return 1500*x[0]**2+4*x[0]*x[1]+x[1]**2\n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a Python function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  grad_x_1=3000*x[0]+4*x[1]\n",
        "  grad_x_2=2*x[1]+4*x[0]\n",
        "  return np.array([grad_x_1,grad_x_2],dtype='float128')\n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\ let \\ A= \\begin{bmatrix}\n",
        "1500 & 2\\\\\n",
        "2 & 1\n",
        "\\end{bmatrix}$ \n",
        "\n",
        "\n",
        " Given that $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x}$ \\\\\n",
        "we can find $\\nabla f(\\mathbf{x}) = 2\\mathbf{A} \\mathbf{x}+2 \\mathbf{b} ----(1) $ \\\\\n",
        "Since we are to minimize  $f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))=g(\\alpha) \\   \\text{with respect to } \\alpha .  \\\\\n",
        "\\text{By calculas we want to minimize} \\ g(\\alpha) so \n",
        "\\\\ \\frac{δg}{δα} = [\\nabla f(x-\\alpha\\nabla f(\\mathbf{x}))]^\\top[-\\nabla f(\\mathbf{x})]   \\\\\n",
        "\\text{By Fermat's theorem, necessary condition for optimality is:} \\\\\n",
        " \\frac{δg}{δα}=0\n",
        " ⇒ \\  [\\nabla f(x-\\alpha\\nabla f(\\mathbf{x}))]^\\top[-\\nabla f(\\mathbf{x})]=0 \\\\ \n",
        "⇒ [-\\nabla f(\\mathbf{x})]^\\top[\\nabla f(x-\\alpha\\nabla f(\\mathbf{x}))]=0  \\ \\ (\\text{taking transpose of both sides} )\\\\\n",
        " ⇒[-\\nabla f(\\mathbf{x})]^\\top[2\\mathbf{A}(\\mathbf{x-α\\nabla}f(\\mathbf{x})+2b]=0 \\ \\  \\text{By Equation (1)}  \\\\ ⇒α= \\frac{[\\nabla f(\\mathbf{x})]^⊤[\\mathbf{A}\\mathbf{x}+\\mathbf{b}]}{[\\nabla f(\\mathbf{x})]^⊤\\mathbf{A}\\nabla f(\\mathbf{x})} \\\\\n",
        " ⇒ α = \\frac{[\\nabla f(\\mathbf{x})]^⊤\\nabla f(\\mathbf{x})}{\\mathbf{2}[\\nabla f(\\mathbf{x})]^⊤\\mathbf{A} \\nabla f(\\mathbf{x})} \\\\ \\text{Now we are to show that function is minimum at the value of  } \\alpha : \\\\ \\nabla^{2}f(x) \\text{ is positive definite matrix so at this value of alpha we get a minimum value of function.}$"
      ],
      "metadata": {
        "id": "CcUJuijXoJK6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf,A,x): #add appropriate arguments to the function\n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(A) is np.ndarray and A.shape[0] == 2 and  A.shape[1] == 2\n",
        "  numerator=np.dot(evalg(x).transpose(),evalg(x))    \n",
        "  denominator=2*np.dot(np.dot(evalg(x).transpose(),A),evalg(x))\n",
        "  step_length=numerator/denominator\n",
        "   \n",
        "  return step_length \n",
        "  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0.\n",
        "  alpha = alpha_start\n",
        "  p=-gradf\n",
        "  while evalf(x+alpha*p)> evalf(x)+gamma*alpha*(np.dot(evalg(x).transpose(),p)):\n",
        "    alpha=alpha*rho\n",
        "  return alpha\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqjdHM3eaHYf"
      },
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x, gradf, direction, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(direction) is np.ndarray and len(direction) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0.\n",
        "  alpha = alpha_start\n",
        "  p=-gradf\n",
        "  while evalf(x-alpha*np.matmul(direction,gradf))> evalf(x)-gamma*alpha*(np.matmul(evalg(x).transpose(),np.matmul(direction,gradf))):\n",
        "    alpha=alpha*rho\n",
        "  return alpha \n",
        "  \n",
        "  \n",
        "  \n",
        "  #Complete the code "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvLRu5s635ph"
      },
      "source": [
        "#line search type \n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdYW5nldqZU-"
      },
      "source": [
        "#complete the code for gradient descent to find the minimizer\n",
        "def find_minimizer_gd(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  \n",
        "  A = np.array([1500,2,2,1]).reshape(2,2)\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A,x) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k \n",
        "  \n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        "\n",
        "def find_minimizer_gdscaling(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  A = np.array([[1500, 2],[2,1]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  d_k=compute_D_k(x)\n",
        "  \n",
        "\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start=args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "        #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x,g_x,d_k, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength      \n",
        "     # raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1   \n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(d_k,evalg(x)))) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    d_k=compute_D_k(x)\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', dir, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k\n",
        "\n",
        "  #Complete the code   "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Question [R]:} \\text{Note down the minimizer and minimum function value of} \\\\  f(x) = 1500x_1^2\n",
        "+ 4x_1x_2 + x_2^2\n",
        ". $"
      ],
      "metadata": {
        "id": "YtIUwX3OJ-TM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Answer: minimizer of function is : (0,0)} \\\\ \\textbf{minimum function value is 0.}$"
      ],
      "metadata": {
        "id": "bE81jO-6KRU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Question [R]:} \\text{ With starting point}  \\\\ \n",
        "x_0= (1,4000) \\\\ \\text{and a stopping tolerance τ = 10−12, find the number of iterations taken\n",
        "by the gradient descent algorithm (without scaling) with exact line search, gradient descent algorithm (without\n",
        "scaling) with backtracking line search, gradient descent algorithm (with scaling) with backtracking line search.} \\\\ \\text{\n",
        "For backtracking line search, use α0 = 1, ρ = 0.5, γ = 0.5. Note the minimizer and minimum objective function\n",
        "value in each case. Comment on your observations.}$"
      ],
      "metadata": {
        "id": "bUaOYiX6Klj9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hoak3jnHkXd"
      },
      "source": [
        "my_start_x = np.array([1.,4000.])\n",
        "my_tol= 1e-12"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Exaxt line search method} $"
      ],
      "metadata": {
        "id": "WUW4pJzwL0gE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgu4yasdJEKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caed6d8b-394c-4a5c-9938-0ec9d15f7cd9"
      },
      "source": [
        "#check gradient descent with exact line search\n",
        "opt_x,number_of_iter=find_minimizer_gd(my_start_x,my_tol,EXACT_LINE_SEARCH)\n",
        "print('minimizer is :',opt_x)\n",
        "print('number of iterations taken :',number_of_iter)\n",
        "print('minimum value of function:',evalf(opt_x))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimizer is : [-7.44595231e-16  4.61544291e-13]\n",
            "number of iterations taken : 14075\n",
            "minimum value of function: 2.1248011077928182463e-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{number of iteration taken to reach optimal  condition:}=14075  \\\\ \\text{minimizer of function:}= [-7.44595231e-16  4.61544291e-13] ≃ [0,0] \\\\ \\text{minimum function value is :}= 2.1248011077928182463e-25 ≃ 0 $"
      ],
      "metadata": {
        "id": "diDuFCk1L_eQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Using backtracking line search method without scalling:}$"
      ],
      "metadata": {
        "id": "IWWIIwtlMklp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0_iOLVoQFYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62eb17e6-4f63-4f93-90f8-28595d6c408d"
      },
      "source": [
        "#check gradient descent with backtracking line search \n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "opt_x_bls,number_of_iter_bls=find_minimizer_gd(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha_start,rho,gamma)\n",
        "print('minimizer is :',opt_x_bls)\n",
        "print('number of iterations taken :',number_of_iter_bls)\n",
        "print('minimum value of function:',evalf(opt_x_bls))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-4.78532202e-16  4.53575301e-13]\n",
            "number of iterations taken : 21985\n",
            "minimum value of function: 2.0520584176089342136e-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{number of iteration taken to reach optimal  condition:}=21985  \\\\ \\text{minimizer of function:}= [-4.78532202e-16  4.53575301e-13] ≃ [0,0] \\\\ \\text{minimum function value is :}= 2.0520584176089342136e-25 ≃ 0 $"
      ],
      "metadata": {
        "id": "8lafdZ-9M0G9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Using backtracking line search method with scalling:} $"
      ],
      "metadata": {
        "id": "49mvzx3aNAf7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpABILpQxPKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0815539-0944-4c20-d82f-1225d6101ac6"
      },
      "source": [
        "#check gradient descent with scaling and backtracking line search \n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "opt_x_bls,number_of_iter_bls=find_minimizer_gdscaling(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha_start,rho,gamma)\n",
        "print('minimizer is :',opt_x_bls)\n",
        "print('number of iterations taken :',number_of_iter_bls)\n",
        "print('minimum value of function:',evalf(opt_x_bls))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{number of iteration taken to reach optimal  condition:}=16  \\\\ \\text{minimizer of function:}= [-2.31674405e-18  9.60515434e-16] ≃ [0,0] \\\\ \\text{minimum function value is :}= 9.217397790469289959e-31 ≃ 0 $"
      ],
      "metadata": {
        "id": "2g4eJVEVNG6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $ \\large \\textbf{Question[R]:}  $ \n",
        "  With starting point $x_0 $= (1, 4000) and τ = 10−12, we will now study the behavior of gradient descent algorithm\n",
        "(without scaling) with backtracking line search, gradient descent algorithm (with scaling) with backtracking\n",
        "line search, for different choices of α0. Take γ = ρ = 0.5. Try $α_0$ ∈ {1, 0.9, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01}.\n",
        "For each α0, record the final minimizer, final objective function value and number of iterations to terminate,\n",
        "for the gradient descent algorithm (without scaling) with backtracking line search and the gradient descent\n",
        "algorithm (with scaling) with backtracking line search. Prepare a plot where the number of iterations for\n",
        "both the algorithms are plotted against α0 values. Use different colors and a legend to distinguish the plots\n",
        "corresponding to the different algorithms. Comment on the observations. Comment about the minimizers and\n",
        "objective function values obtained for different choices of the α0 values for the two algorithms."
      ],
      "metadata": {
        "id": "IZBBmZOyNfKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_values=[1.0,0.9,0.75,0.6,0.5,0.4,0.25,0.1,0.01] \n",
        "my_start_x=np.array([1,4000])\n",
        "my_tol= 1/10**(12)\n",
        "rho=0.5\n",
        "gamma=0.5\n",
        "iter_list=[]\n",
        "iter_list_scl=[]\n",
        "for alpha in alpha_values:\n",
        "  print('when alpha is :',alpha)\n",
        "  print('\\n----------------------------------------------------------------------------\\n')\n",
        "  print('backtracking line search method without scalling')\n",
        "  opt_x_bls,number_of_iter_bls=find_minimizer_gd(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha,rho,gamma)\n",
        "  print('minimizer is :',opt_x_bls)\n",
        "  print('number of iterations taken :',number_of_iter_bls)\n",
        "  print('minimum value of function:',evalf(opt_x_bls))\n",
        "  iter_list.append(number_of_iter_bls)\n",
        "  print('\\n----------------------------------------------------------------------------\\n')\n",
        "  print('backtracking line search method with scalling')\n",
        "  opt_x_bls,number_of_iter_bls=find_minimizer_gdscaling(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha_start,rho,gamma)\n",
        "  print('minimizer is :',opt_x_bls)\n",
        "  print('number of iterations taken :',number_of_iter_bls)\n",
        "  print('minimum value of function:',evalf(opt_x_bls))\n",
        "  iter_list_scl.append(number_of_iter_bls)\n",
        "  print('\\n----------------------------------------------------------------------------\\n')\n",
        "  print('\\n----------------------------------------------------------------------------\\n')\n",
        "print(iter_list)  \n",
        "print(iter_list_scl)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0CWb7GqKQ5n",
        "outputId": "ecb42ec2-c02d-4d21-cb69-25898d57dfa0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when alpha is : 1.0\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-4.78532202e-16  4.53575301e-13]\n",
            "number of iterations taken : 21985\n",
            "minimum value of function: 2.0520584176089342136e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when alpha is : 0.9\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 0.9 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-5.48799355e-16  4.71515450e-13]\n",
            "number of iterations taken : 15941\n",
            "minimum value of function: 2.2174352131748196821e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when alpha is : 0.75\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 0.75 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-7.18643891e-16  4.89175378e-13]\n",
            "number of iterations taken : 6750\n",
            "minimum value of function: 2.3866105285479558722e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when alpha is : 0.6\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 0.6 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-6.07554226e-16  4.94036636e-13]\n",
            "number of iterations taken : 6887\n",
            "minimum value of function: 2.4342526428386636246e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when alpha is : 0.5\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 0.5 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-4.78532202e-16  4.53575301e-13]\n",
            "number of iterations taken : 21985\n",
            "minimum value of function: 2.0520584176089342136e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when alpha is : 0.4\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 0.4 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-7.24091328e-16  4.84454819e-13]\n",
            "number of iterations taken : 11362\n",
            "minimum value of function: 2.340797761558255364e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when alpha is : 0.25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 0.25 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-4.78532202e-16  4.53575301e-13]\n",
            "number of iterations taken : 21985\n",
            "minimum value of function: 2.0520584176089342136e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when alpha is : 0.1\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 0.1 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-7.24091328e-16  4.84454819e-13]\n",
            "number of iterations taken : 11362\n",
            "minimum value of function: 2.340797761558255364e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when alpha is : 0.01\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 0.01 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-5.80852788e-16  4.86459735e-13]\n",
            "number of iterations taken : 5509\n",
            "minimum value of function: 2.3601891237044221942e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "[21985, 15941, 6750, 6887, 21985, 11362, 21985, 11362, 5509]\n",
            "[16, 16, 16, 16, 16, 16, 16, 16, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(alpha_values,iter_list,color='r')\n",
        "plt.plot(alpha_values,iter_list_scl,color='b')\n",
        "plt.xlabel('alpha values')\n",
        "plt.ylabel('number of iterations')\n",
        "plt.legend(['without scalling','with scalling'],bbox_to_anchor=(1.50, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "E6Z0FmaQPU_6",
        "outputId": "075b62cd-69dc-4755-d111-c16a8fe83abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Observations on iterations taken to reach optimality:}\\text{As we can see that number of iterations taken to reach optimality without} \\\\ \\large \\text{ scalling does not follow any specific pattern but as the alpha value decrease to zero then number of iterations decreases so fast. but in compare } \\\\ \\large \\text{ of without scalling if we talk about the backward line search method using scalling always take 16 iterations to reach optimal condition } \\\\ \\large \\text{ so we can conclude that after scalling our gradient descent is taking a low number of iterations to reach optimal condition.} $"
      ],
      "metadata": {
        "id": "7u0Jdfb-SnYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\large \\textbf{Observations  on minimizers and objective function value:}  \\\\ \\text{As we can see that } x_1 \\text{is tending to zero from negative side as iterations occurs and }  x_2 \\text{ tends to zero from positive side as iterations occures } \\\\ \\text{and we can see that for all values all alpha we get a solution which is almost equal to (0,0) and minumum function value is also} \\\\ \\text{ approximately equal to zero.  }$"
      ],
      "metadata": {
        "id": "Rd6Qk2ga52GY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Question:} $ \\\\ R] With starting point x0 = (1, 4000) and τ = 10−12, we will now study the behavior of gradient descent algorithm\n",
        "(without scaling) with backtracking line search, gradient descent algorithm (with scaling) with backtracking\n",
        "line search, for different choices of ρ. Take α = 1, γ = 0.5. Try ρ ∈ {0.9, 0.8, 0.75, 0.6, 0.5, 0.4, 0.25, 0.1, 0.01}.\n",
        "For each ρ, record the final minimizer, final objective function value and number of iterations to terminate,\n",
        "for the gradient descent algorithm (without scaling) with backtracking line search and the gradient descent\n",
        "algorithm (with scaling) with backtracking line search. Prepare a plot where the number of iterations for\n",
        "both the algorithms are plotted against ρ values. Use different colors and a legend to distinguish the plots\n",
        "corresponding to the different algorithms. Comment on the observations. Comment about the minimizers and\n",
        "objective function values obtained for different choices of the ρ values for both the algorithms."
      ],
      "metadata": {
        "id": "isc5kEAiUh7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rho_values=[0.01,0.1,0.25,0.4,0.5,0.6,0.75,0.8,0.9]\n",
        "my_start_x=np.array([1,4000])\n",
        "my_tol= 1/10**(12)\n",
        "alpha=1.0\n",
        "gamma=0.5\n",
        "iter_list=[]\n",
        "iter_list_scl=[]\n",
        "for rho in rho_values:\n",
        "  print('when rho is :',rho)\n",
        "  print('\\n----------------------------------------------------------------------------\\n')\n",
        "  print('backtracking line search method without scalling')\n",
        "  opt_x_bls,number_of_iter_bls=find_minimizer_gd(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha,rho,gamma)\n",
        "  print('minimizer is :',opt_x_bls)\n",
        "  print('number of iterations taken :',number_of_iter_bls)\n",
        "  print('minimum value of function:',evalf(opt_x_bls))\n",
        "  iter_list.append(number_of_iter_bls)\n",
        "  print('\\n----------------------------------------------------------------------------\\n')\n",
        "  print('backtracking line search method with scalling')\n",
        "  opt_x_bls,number_of_iter_bls=find_minimizer_gdscaling(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha_start,rho,gamma)\n",
        "  print('minimizer is :',opt_x_bls)\n",
        "  print('number of iterations taken :',number_of_iter_bls)\n",
        "  print('minimum value of function:',evalf(opt_x_bls))\n",
        "  iter_list_scl.append(number_of_iter_bls)\n",
        "  print('\\n----------------------------------------------------------------------------\\n')\n",
        "  print('\\n----------------------------------------------------------------------------\\n')\n",
        "print(iter_list)  \n",
        "print(iter_list_scl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLcLGGGbRdOU",
        "outputId": "a746abd0-51e8-46e2-f308-6b132161af14"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when rho is : 0.01\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.01  gamma: 0.5\n",
            "minimizer is : [-6.03891924e-16  4.89972672e-13]\n",
            "number of iterations taken : 17463\n",
            "minimum value of function: 2.3943668542911699835e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-5.88632151e-16  4.39928337e-13]\n",
            "number of iterations taken : 130\n",
            "minimum value of function: 1.9302084997635642937e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when rho is : 0.1\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.1  gamma: 0.5\n",
            "minimizer is : [-4.35224428e-16  4.42644725e-13]\n",
            "number of iterations taken : 45435\n",
            "minimum value of function: 1.954478834023159006e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-6.22892805e-16  4.51919078e-13]\n",
            "number of iterations taken : 23\n",
            "minimum value of function: 2.0368685744853979037e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when rho is : 0.25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.25  gamma: 0.5\n",
            "minimizer is : [-5.67128954e-16  4.70177132e-13]\n",
            "number of iterations taken : 24570\n",
            "minimum value of function: 2.2048238422067500588e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-6.9534609e-16  4.5561674e-13]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 2.0704462793405916438e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when rho is : 0.4\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.4  gamma: 0.5\n",
            "minimizer is : [-5.16461460e-16  4.67846629e-13]\n",
            "number of iterations taken : 20049\n",
            "minimum value of function: 2.1831406770108283949e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.10920879e-18  1.38155681e-15]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 1.9037164073888824085e-30\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when rho is : 0.5\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.5  gamma: 0.5\n",
            "minimizer is : [-4.78532202e-16  4.53575301e-13]\n",
            "number of iterations taken : 21985\n",
            "minimum value of function: 2.0520584176089342136e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.31674405e-18  9.60515434e-16]\n",
            "number of iterations taken : 16\n",
            "minimum value of function: 9.217397790469289959e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when rho is : 0.6\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.6  gamma: 0.5\n",
            "minimizer is : [-7.07691748e-16  4.70469643e-13]\n",
            "number of iterations taken : 15834\n",
            "minimum value of function: 2.207611361654380832e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-2.68496827e-18  1.53311688e-15]\n",
            "number of iterations taken : 15\n",
            "minimum value of function: 2.3447954703965862527e-30\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when rho is : 0.75\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.75  gamma: 0.5\n",
            "minimizer is : [-7.15454842e-16  4.83616543e-13]\n",
            "number of iterations taken : 3135\n",
            "minimum value of function: 2.3326875114120646613e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-3.59593964e-18  9.57478862e-16]\n",
            "number of iterations taken : 15\n",
            "minimum value of function: 9.223897999650186346e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when rho is : 0.8\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.8  gamma: 0.5\n",
            "minimizer is : [-6.99968590e-16  4.40149178e-13]\n",
            "number of iterations taken : 1471\n",
            "minimum value of function: 1.9323387030718191021e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-3.89959677e-18  7.65599523e-16]\n",
            "number of iterations taken : 15\n",
            "minimum value of function: 5.9701079435151899295e-31\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "when rho is : 0.9\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method without scalling\n",
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.9  gamma: 0.5\n",
            "minimizer is : [-6.44686128e-16  4.90962904e-13]\n",
            "number of iterations taken : 127\n",
            "minimum value of function: 2.404019351870076014e-25\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "backtracking line search method with scalling\n",
            "minimizer is : [-1.90920422e-16  3.38018326e-15]\n",
            "number of iterations taken : 14\n",
            "minimum value of function: 6.3520166220020603727e-29\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "\n",
            "[17463, 45435, 24570, 20049, 21985, 15834, 3135, 1471, 127]\n",
            "[130, 23, 16, 16, 16, 15, 15, 15, 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(rho_values,iter_list,color='r')\n",
        "plt.plot(rho_values,iter_list_scl,color='b')\n",
        "plt.xlabel('rho values')\n",
        "plt.ylabel('number of iterations')\n",
        "plt.legend(['without scalling','with scalling'],bbox_to_anchor=(1.50, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "-Kl46FYbVF-H",
        "outputId": "3cb60da4-16f0-4a17-f2bd-13880297a1b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Observations on iterations taken to reach optimality:} \\\\ \\text{As we can see that if we talk about backtracking line search method without scalling then it is clear from curve that as soon as} \\\\ \\text{  rho value increases and becomes closer to 1 then number of iteration decreases fastly and at value of rho 0.9 it becomes 126 iterations} \\\\ \\text{But if we talk about backtracking linear search method with scalling has almost same number of iterations for all rho values.} \\\\ \\text{ For value of rho = 0.01 number of iterations are 130 which are higher than other iterations taken to reach optimality condition for }\\\\ \\text{  other rho values. but for other values of rho number of iterations are same. } $"
      ],
      "metadata": {
        "id": "0fJGVART66FN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\large \\textbf{Observations  on minimizers and objective function value:}  \\\\ \\text{As we can see that } x_1 \\text{is tending to zero from negative side as iterations occurs and }  x_2 \\text{ tends to zero from positive side as iterations occures } \\\\ \\text{and we can see that for all values all alpha we get a solution which is almost equal to (0,0) and minumum function value is also} \\\\ \\text{ approximately equal to zero.  }$"
      ],
      "metadata": {
        "id": "OE8K1qG39Dlp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hL-RNElcVdf7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}