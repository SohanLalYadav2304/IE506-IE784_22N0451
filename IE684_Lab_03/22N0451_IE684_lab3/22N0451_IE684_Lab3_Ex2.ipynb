{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Uvr4vcsE-AIF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Excercise 2:} \\ $"
      ],
      "metadata": {
        "id": "OUxZASCD-Lm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{The function is:} \\ \\  f(x)=512(x_2-x_1^2)^2+(4-x_1)^2 $"
      ],
      "metadata": {
        "id": "pp43aK4r_eaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{Question 1}$ \\\\ [R] Can you design a suitable diagonal matrix Dk for gradient descent algorithm with scaling to solve minx q(x).\n",
        "If you can come up with a suitable choice of Dk, use it in the implementation of Algorithm 1 (with backtracking\n",
        "line search) to find the minimizer of q(x) with starting point x0 = (8, 8) and τ = 10−5. Consider α0 = 1, ρ =\n",
        "0.5, γ = 0.5 for backtracking line search. Comment on your observations when compared to the gradient descent\n",
        "(without scaling) with backtracking line search. If you cannot find a suitable choice of Dk, explain clearly the\n",
        "reasons."
      ],
      "metadata": {
        "id": "cLqE3O3s_mdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Answer:}\\text{as we derived in last excercise that our } D_k \\text{ is :}$ "
      ],
      "metadata": {
        "id": "Y_9QkYNt_uJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " $D_k=[(\\frac{\\delta^2f}{\\delta x_1^2})^{-1},(\\frac{\\delta^2f}{\\delta x_2^2})^{-1} , (\\frac{\\delta^2f}{\\delta x_3^2})^{-1}, ...............(\\frac{\\delta^2f}{\\delta x_n^2})^{-1}] $"
      ],
      "metadata": {
        "id": "spsRvVeUENmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{So according to question Dk is:} \\\\ $ $  D_k= \\begin{bmatrix}\n",
        "(2+6144*x_1^2-2048*x_2)^{-1} & 0\\\\\n",
        "0 & (1024)^{-1}\n",
        "\\end{bmatrix}$"
      ],
      "metadata": {
        "id": "dpnZ3eG4EoCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{our hessian matrix can be written as:} \\ $\n",
        "\n",
        "$H_x=\\nabla^2f(x)=$\\begin{bmatrix}\n",
        "(2+6144*x_1^2-2048*x_2)^{-1} & 0\\\\\n",
        "0 & (1024)^{-1}\n",
        "\\end{bmatrix}$"
      ],
      "metadata": {
        "id": "H1a6MrCmFH52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evalf(x):\n",
        "  assert type(x) is np.ndarray and len(x)==2\n",
        "  return 512*(x[1]-(x[0]**2))**2+(4-x[0])**2"
      ],
      "metadata": {
        "id": "Hncue3H--KqY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalg(x):\n",
        "  assert type(x) is np.ndarray and len(x)==2\n",
        "  x1_grad=2048*x[0]**3-2048*x[0]*x[1]-8+2*x[0]\n",
        "  x2_grad=1024*x[1]-1024*(x[0]**2)\n",
        "  return np.array([x1_grad,x2_grad],dtype='float64')"
      ],
      "metadata": {
        "id": "kcPGzqQuGDdX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalh(x):\n",
        "  assert type(x) is np.ndarray and len(x)==2\n",
        "  df_dxx=6144*(x[0]**2)-2048*x[1]+2\n",
        "  df_dyx=-2048*x[0]\n",
        "  df_dyy=1024\n",
        "  return np.array([df_dxx,df_dyx,df_dyx,df_dyy],dtype='float64').reshape(2,2)\n",
        "\n"
      ],
      "metadata": {
        "id": "iXCVmW_7Gmap"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evalh(np.array([8,8]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1IT3IPV73zR",
        "outputId": "7275f517-a6b6-488f-f096-b9c5588dc67a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[376834., -16384.],\n",
              "       [-16384.,   1024.]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_D_k(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  Hes=evalh(x)\n",
        "  D_k=np.array([1/Hes[0,0],0,0,1/Hes[1,1]]).reshape(2,2)\n",
        "  return D_k"
      ],
      "metadata": {
        "id": "IZtHi5WqHuVV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0.\n",
        "  alpha = alpha_start\n",
        "  p=-gradf\n",
        "  while evalf(x+alpha*p)> evalf(x)+gamma*alpha*(np.dot(evalg(x).transpose(),p)):\n",
        "    alpha=alpha*rho\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "xF6RaqTwKwQB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_condition_number(A):\n",
        "  assert type(A) is np.ndarray\n",
        "  assert A.shape[0] == A.shape[1]\n",
        "  eigen_values=np.linalg.eig(A)[0]\n",
        "  return max(eigen_values)/min(eigen_values)"
      ],
      "metadata": {
        "id": "1xSFR6cYM2q_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x, gradf, direction, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(direction) is np.ndarray and len(direction) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0.\n",
        "  alpha = alpha_start\n",
        "  p=-gradf\n",
        "  while evalf(x-alpha*np.matmul(direction,gradf))> evalf(x)-gamma*alpha*(np.matmul(evalg(x).transpose(),np.matmul(direction,gradf))):\n",
        "    alpha=alpha*rho\n",
        "  return alpha \n",
        "  "
      ],
      "metadata": {
        "id": "6V9vfNKjK6kP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "metadata": {
        "id": "gq87EI75LUep"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_q(d_k,hes):\n",
        "  viv=np.matmul(scipy.linalg.sqrtm(d_k).T,np.matmul(hes,scipy.linalg.sqrtm(d_k)))\n",
        "  return viv"
      ],
      "metadata": {
        "id": "xmX3fWIj9f1A"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the code for gradient descent to find the minimizer\n",
        "def find_minimizer_gd(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  \n",
        "  A = np.array([1500,2,2,1]).reshape(2,2)\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  cond=[]\n",
        "  cond_hess=[]\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A,x) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    cond_hess.append(find_condition_number(evalh(x)))\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k,cond_hess\n",
        "  \n"
      ],
      "metadata": {
        "id": "vkgqHifyQwvV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        "\n",
        "def find_minimizer_gdscaling(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  A = np.array([[1500, 2],[2,1]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  d_k=compute_D_k(x)\n",
        "  condition=find_condition_number(find_q(d_k,evalh(x)))\n",
        "  condition1=find_condition_number(evalh(x))\n",
        "  cond1=[]\n",
        "  cond2=[]\n",
        "  cond1.append(condition)\n",
        "  cond2.append(condition1)\n",
        "  \n",
        "\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start=args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "        #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x,g_x,d_k, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength      \n",
        "     # raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1   \n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(d_k,evalg(x)))) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    d_k=compute_D_k(x)\n",
        "    cond1.append(condition)\n",
        "    cond2.append(condition1)\n",
        "    \n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', dir, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k,cond1,cond2\n",
        "\n",
        "  #Complete the code   "
      ],
      "metadata": {
        "id": "_ecTbJyDQ29z"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([8,8])\n",
        "my_tol=1e-5\n",
        "alpha=1.0\n",
        "rho=0.5\n",
        "gamma=0.5\n",
        "opt_x_bls,number_of_iter_bls,condition_no_hess=find_minimizer_gd(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha,rho,gamma)\n",
        "print('minimizer is :',opt_x_bls)\n",
        "print('number of iterations taken :',number_of_iter_bls)\n",
        "print('minimum value of function:',evalf(opt_x_bls))\n",
        "\n",
        "print('\\n-------------------------------------------------------------------\\n')\n",
        "print('Backtracking using scalling')\n",
        "opt_x_bls,number_of_iter_blscondition_no_with_sc,condition_no_Q,condition_no_hes_scl=find_minimizer_gdscaling(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha,rho,gamma)\n",
        "print('minimizer is :',opt_x_bls)\n",
        "print('number of iterations taken :',number_of_iter_bls)\n",
        "print('minimum value of function:',evalf(opt_x_bls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuAsAjItMN6s",
        "outputId": "cc2551c8-b0cc-4221-f300-5a6e4817a5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params for Backtracking LS: alpha start: 1.0 rho: 0.5  gamma: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Observations:} $ Here we can see that number of iterations taken to reach to optimal condition for both conditions are almost equal so in this problem we see that scalling in this way doesn't give us any benefit to use. "
      ],
      "metadata": {
        "id": "aNFEHzEVNxYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Question:}$[R] Based on our discussion on condition number and the derivation of the gradient descent scheme with\n",
        "scaling, can you identify and write down the matrix Q whose condition number needs to be analyzed in the\n",
        "new gradient descent scheme with scaling with Dk = (∇2f(xk))−1?"
      ],
      "metadata": {
        "id": "hfJgetroOqZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Answer:} \\text{as we know that} \\  \\mathbf{Q} = \\mathbf{(D^k)}^{\\frac{1}{2}}H^k \\mathbf{(D^k)}^{\\frac{1}{2}}  $"
      ],
      "metadata": {
        "id": "UCRunGXhTPDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where $H^k$ is Hassian matrix of a function for $k^{th}$ iteration"
      ],
      "metadata": {
        "id": "tg5WcLGdTmUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{Question:} $"
      ],
      "metadata": {
        "id": "fHryzroAZ68X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[R] With starting point x0 = (8, 8) and a stopping tolerance τ = 10−5, find the number of iterations taken\n",
        "by the gradient descent algorithm (without scaling) with backtracking line search, gradient descent algorithm\n",
        "(with scaling) with backtracking line search. For backtracking line search, use α0 = 1, ρ = 0.5, γ = 0.5.\n",
        "Note the minimizer and minimum objective function value in each case. Comment on your observations.\n",
        "Also note the condition number of the Hessian matrix involved in the gradient descent algorithm (without\n",
        "scaling) with backtracking line search and condition number of the matrix Q involved in the gradient descent\n",
        "algorithm (with scaling) with backtracking line search in each iteration. Prepare a plot depicting the behavior\n",
        "of condition numbers in both algorithms against iterations. Use different colors and legend to distinguish the\n",
        "methods. Comment on your observations."
      ],
      "metadata": {
        "id": "EW4UR7nWZ5nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\large \\textbf{Answer:}$ As we can see that we get minimizer of function is obtained approximately (4,16) and minimum function value is zero obtained.\n",
        " "
      ],
      "metadata": {
        "id": "iJjv--PAfmmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(condition_no_hess,label='condition number of hessian matrix with respect to iterations without scalling')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('condition number')\n",
        "plt.legend(bbox_to_anchor=(1.50,0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8gxLbut6eZVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{as we can see that as soon as iterations occures condition number of hessian matrix also increases which clearly indicates that illness of problem increases.}$"
      ],
      "metadata": {
        "id": "fwP1z8GOggYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(condition_no_Q,label='condition number of Q matrix with respect to iterations with scalling')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('condition number')\n",
        "plt.legend(bbox_to_anchor=(1.50,0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xPUCTxQhfYnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_k_newton(x):\n",
        "  assert type(x) is np.ndarray and len(x)==2\n",
        "  return np.linalg.inv(evalh(x))"
      ],
      "metadata": {
        "id": "l3QoRLifUGRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.linalg"
      ],
      "metadata": {
        "id": "BQNVrDEjDDeN"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#complete the code for gradient descent with scaling to find the minimizer\n",
        "\n",
        "def find_minimizer_gdscaling_newton(start_x, tol, line_search_type,*args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  A = np.array([[1500, 2],[2,1]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  hes=evalh(x)\n",
        "  d_k=d_k_newton(x)\n",
        "  condition=find_condition_number(find_q(d_k,evalh(x)))\n",
        "  condition_num=[]\n",
        "  condition_num.append(condition)\n",
        "  \n",
        "\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start=args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "        #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking_scaled_direction(x,g_x,d_k, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength      \n",
        "     # raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1   \n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,np.matmul(d_k,evalg(x)))) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    d_k=d_k_newton(x)\n",
        "    condition_num.append(find_condition_number(find_q(d_k,evalh(x))))\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', dir, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x,k,condition_num\n",
        "\n",
        "  #Complete the code   "
      ],
      "metadata": {
        "id": "sx59WDotVIXw"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_x_bls,number_of_iter_bls,con=find_minimizer_gdscaling_newton(my_start_x,my_tol,BACKTRACKING_LINE_SEARCH,alpha,rho,gamma)\n",
        "print('minimizer is :',opt_x_bls)\n",
        "print('number of iterations taken :',number_of_iter_bls)\n",
        "print('minimum value of function:',evalf(opt_x_bls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XSodzT1VP9S",
        "outputId": "d1f989da-5d97-434c-caa8-b81b0163dcaa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimizer is : [ 4. 16.]\n",
            "number of iterations taken : 48\n",
            "minimum value of function: 9.49387788546246e-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(con)"
      ],
      "metadata": {
        "id": "fiOxe3k83cg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(con,label='condition number')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('condition number')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "MBsrduMqVXq5",
        "outputId": "dce593a8-7f8d-4793-8b5c-25bc7b82fc46"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{As above diagram indicates that as soon as iterations occures condition number of our matrix Q decreases }$"
      ],
      "metadata": {
        "id": "kJzZahAxgDph"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5Qfe9t5ZqEH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}