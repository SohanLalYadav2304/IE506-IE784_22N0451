{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 1. Exercise 2. }$\n",
        "\n",
        "Now we will consider a slightly different algorithm which can be used to find a minimizer of the function $f(\\mathbf{x})=f(x_1,x_2)= (x_1+100)^2 + (x_2-25)^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpe6eGRLvSh"
      },
      "source": [
        "$\\textbf{[R]}$ Write the function $f(\\mathbf{x})$ in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{x}\\in {\\mathbb{R}}^2$, $\\mathbf{A}$ is a symmetric matrix of size $2 \\times 2$, $\\mathbf{b}\\in{\\mathbb{R}}^2$ and $c\\in\\mathbb{R}$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "WEKxfLCWNj5Q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalf2(x):\n",
        "  assert A.shape==(2,2) and type(A) is ndarray and len(b)==2 and type(b) is ndarray and len(x)==2 and type(x) is ndarray\n",
        "  return np.dot(x.transpose,(np.dot(A,x)))+2*np.dot(b.transpose(),x)+c"
      ],
      "metadata": {
        "id": "Zt8fF99GNFW3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPeLBt0L7F7"
      },
      "source": [
        "$\\large \\textbf{ANSWER:}$ Write your answer here:\n",
        "\n",
        "$f(x_1,x_2)=\\begin{bmatrix} \n",
        "\tx_1 & x_2 \\\\\n",
        "\t\\end{bmatrix}\n",
        "\\begin{bmatrix} \n",
        "\t1 & 0  \\\\\n",
        "\t0 & 1 \\\\\n",
        "\t\\end{bmatrix}\n",
        "  \\begin{bmatrix} \n",
        "\tx_1 \\\\\n",
        "\tx_2 \\\\\n",
        "\t\\end{bmatrix}\n",
        "  + 2*\n",
        "  \\begin{bmatrix} \n",
        "\t100 & -25 \\\\\n",
        "\t\\end{bmatrix}\n",
        "  \\begin{bmatrix} \n",
        "\tx_1 \\\\\n",
        "\tx_2 \\\\\n",
        "\t\\end{bmatrix}\n",
        "  +10625$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjANnIQ3L39D"
      },
      "source": [
        "\n",
        "$\\textbf{[R]}$ It turns out that for a function $f:{\\mathbb{R}}^n\\rightarrow \\mathbb{R}$ of the form $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$, the analytical solution to $\\min_{\\alpha \\geq 0} f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ can be found in closed form. Find the solution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jU-adJ0L-P1"
      },
      "source": [
        "$\\large \\textbf{ANSWER:}$\n",
        "Write your answer here:\n",
        " Given that $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$ \\\\\n",
        "we can find $\\nabla f(\\mathbf{x}) = 2\\mathbf{A} \\mathbf{x}+2 \\mathbf{b} ----(1) $ \\\\\n",
        "Since we are to minimize  $f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))=g(\\alpha) \\   \\text{with respect to } \\alpha .  \\\\\n",
        "\\text{By calculas we want to minimize} \\ g(\\alpha) so \n",
        "\\\\ \\frac{δg}{δα} = [\\nabla f(x-\\alpha\\nabla f(\\mathbf{x}))]^\\top[-\\nabla f(\\mathbf{x})]   \\\\\n",
        "\\text{By Fermat's theorem, necessary condition for optimality is:} \\\\\n",
        " \\frac{δg}{δα}=0\n",
        " ⇒ \\  [\\nabla f(x-\\alpha\\nabla f(\\mathbf{x}))]^\\top[-\\nabla f(\\mathbf{x})]=0 \\\\ \n",
        "⇒ [-\\nabla f(\\mathbf{x})]^\\top[\\nabla f(x-\\alpha\\nabla f(\\mathbf{x}))]=0  \\ \\ (\\text{taking transpose of both sides} )\\\\\n",
        " ⇒[-\\nabla f(\\mathbf{x})]^\\top[2\\mathbf{A}(\\mathbf{x-α\\nabla}f(\\mathbf{x})+2b]=0 \\ \\  \\text{By Equation (1)}  \\\\ ⇒α= \\frac{[\\nabla f(\\mathbf{x})]^⊤[\\mathbf{A}\\mathbf{x}+\\mathbf{b}]}{[\\nabla f(\\mathbf{x})]^⊤\\mathbf{A}\\nabla f(\\mathbf{x})} \\\\\n",
        " ⇒ α = \\frac{[\\nabla f(\\mathbf{x})]^⊤\\nabla f(\\mathbf{x})}{\\mathbf{2}[\\nabla f(\\mathbf{x})]^⊤\\mathbf{A} \\nabla f(\\mathbf{x})} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "We will use this idea to construct a suitable step length finding procedure for our modified algorithm given below: \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "&\\textbf{While } \\| \\nabla f(\\mathbf{x}^k) \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^k - \\eta^k \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  return (x[0]+100)**2 + (x[1]-25)**2\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  return np.array([2*(x[0]+100),2*(x[1]-25)])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A=np.array([1,0,0,1]).reshape((2,2))\n",
        "b=np.array([100,-25]).reshape((2,1))"
      ],
      "metadata": {
        "id": "_I4htpuF8nmk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength\n",
        "def compute_steplength(x,A): #add appropriate arguments to the function\n",
        "  numerator=np.dot(evalg(x).transpose(),evalg(x))    \n",
        "  denominator=2*np.dot(np.dot(evalg(x).transpose(),A),evalg(x))\n",
        "  step_length=numerator/denominator\n",
        "   \n",
        "  return step_length"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  k = 0\n",
        "  print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength(x,A) #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  print(k) \n",
        "  return x \n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2edc0057-201a-4947-8511-f07a4b149c17"
      },
      "source": [
        "my_start_x = np.array([10,10])\n",
        "my_tol= 1e-3\n",
        "find_minimizer(my_start_x, my_tol)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-100.,   25.])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{[R]}$ With starting point $x_0 = (10, 10)$ and the new module to compute $η^k$\n",
        ", try $τ = 10^{−p}$ where p = 1, 2, . . . , 10.\n",
        "For each τ , record the number of iterations taken by the algorithm to terminate. \\\\\n",
        "Prepare a plot where the number of iterations is plotted against τ values. \\\\\n",
        " Compare and contrast the plot with the plots obtained in\n",
        "Exercise 1 with fixed step length values.\n"
      ],
      "metadata": {
        "id": "E3VCc4lW1l4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_1(start_x, tol):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  k =0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength(x,A) #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k=k+1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  print(k)\n",
        "  return x,k"
      ],
      "metadata": {
        "id": "LvVawIMZ9_9z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tol_values=[]\n",
        "iterations=[]\n",
        "for p in range(1,11):\n",
        "  my_start_x = np.array([10,10])\n",
        "  my_tol= (10)**(-p)\n",
        "  print('when tol is',my_tol)\n",
        "  tol_values.append(my_tol)\n",
        "  minimizer,number_of_iterations=find_minimizer_1(my_start_x, my_tol)\n",
        "  print('minimizer is:',minimizer)\n",
        "  print('number_of_iterations',number_of_iterations)\n",
        "  iterations.append(number_of_iterations)\n",
        "print(tol_values)\n",
        "print(iterations)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HaP1eHf9T8f",
        "outputId": "3da8295f-db93-4b1c-f000-7714c5a60322"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when tol is 0.1\n",
            "1\n",
            "minimizer is: [-100.   25.]\n",
            "number_of_iterations 1\n",
            "when tol is 0.01\n",
            "1\n",
            "minimizer is: [-100.   25.]\n",
            "number_of_iterations 1\n",
            "when tol is 0.001\n",
            "1\n",
            "minimizer is: [-100.   25.]\n",
            "number_of_iterations 1\n",
            "when tol is 0.0001\n",
            "1\n",
            "minimizer is: [-100.   25.]\n",
            "number_of_iterations 1\n",
            "when tol is 1e-05\n",
            "1\n",
            "minimizer is: [-100.   25.]\n",
            "number_of_iterations 1\n",
            "when tol is 1e-06\n",
            "1\n",
            "minimizer is: [-100.   25.]\n",
            "number_of_iterations 1\n",
            "when tol is 1e-07\n",
            "1\n",
            "minimizer is: [-100.   25.]\n",
            "number_of_iterations 1\n",
            "when tol is 1e-08\n",
            "1\n",
            "minimizer is: [-100.   25.]\n",
            "number_of_iterations 1\n",
            "when tol is 1e-09\n",
            "1\n",
            "minimizer is: [-100.   25.]\n",
            "number_of_iterations 1\n",
            "when tol is 1e-10\n",
            "1\n",
            "minimizer is: [-100.   25.]\n",
            "number_of_iterations 1\n",
            "[0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09, 1e-10]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft_3BxMzfREx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "62e2481d-a875-48f1-dec2-4c5769e41c9d"
      },
      "source": [
        "plt.plot(tol_values,iterations,color='b')\n",
        "plt.scatter(tol_values,iterations,color='r')\n",
        "plt.title('tol_value vs number of iterations')\n",
        "plt.xlabel('tol_values')\n",
        "plt.ylabel('number of iterations')\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\textbf{ANSWER:}$ As we can see in above plot that number of iterations does not depend on tolerance value when we use Gradient Descent Procedure with line search to compute step length but in excercise 1 we were fixing step length 0.1 and due to that number of iterations were depending over tolerance values and number of iterations were increasing very fast when tolerance value decreases. SO we can conclude that Gradient Descent Procedure with line search to compute step length is better then Gradient Descent Procedure with constant step length.  "
      ],
      "metadata": {
        "id": "aP_AQ2CP2MHl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}